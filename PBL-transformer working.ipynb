{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12153,"status":"ok","timestamp":1768301768026,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"k04IalKq5NWX","outputId":"363169b0-98e9-4c6d-bb6f-414bd6dccd30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lifelines\n","  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: umap-learn in /usr/local/lib/python3.12/dist-packages (0.5.9.post2)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from lifelines) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from lifelines) (1.16.3)\n","Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.12/dist-packages (from lifelines) (2.2.2)\n","Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.12/dist-packages (from lifelines) (3.10.0)\n","Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.12/dist-packages (from lifelines) (1.8.0)\n","Collecting autograd-gamma>=0.3 (from lifelines)\n","  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting formulaic>=0.2.2 (from lifelines)\n","  Downloading formulaic-1.2.1-py3-none-any.whl.metadata (7.0 kB)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (0.60.0)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (0.6.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from umap-learn) (4.67.1)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n","Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines)\n","  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: narwhals>=1.17 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.2.2->lifelines) (2.15.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.2.2->lifelines) (4.15.0)\n","Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.2.2->lifelines) (2.0.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (4.61.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (3.3.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (2.9.0.post0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1->lifelines) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1->lifelines) (2025.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.17.0)\n","Downloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading formulaic-1.2.1-py3-none-any.whl (117 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m117.3/117.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n","Building wheels for collected packages: autograd-gamma\n","  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4030 sha256=f7ee4ccfeda4e119b920b6fdb3b01bf3a87c4b62c2835146e0c497d40227b001\n","  Stored in directory: /root/.cache/pip/wheels/50/37/21/0a719b9d89c635e89ff24bd93b862882ad675279552013b2fb\n","Successfully built autograd-gamma\n","Installing collected packages: interface-meta, autograd-gamma, formulaic, lifelines\n","Successfully installed autograd-gamma-0.5.0 formulaic-1.2.1 interface-meta-1.3.0 lifelines-0.30.0\n"]}],"source":["!pip install lifelines scikit-learn umap-learn openpyxl\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5499,"status":"ok","timestamp":1768301778217,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"A_eSrV6V5Ufa"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","from lifelines.utils import concordance_index\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49328,"status":"ok","timestamp":1768301827551,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"OlxHdSYK5c6z","outputId":"d0c89f28-10f9-486f-f48e-73de71650815"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1244,"status":"ok","timestamp":1768301828800,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"iGpGuZIv5oeb"},"outputs":[],"source":["BASE = \"/content/drive/MyDrive/personalised survival treatment\"\n","\n","# Clinical (EXCEL + processed)\n","EXCEL_PATH = os.path.join(\n","    BASE,\n","    \"I-SPY-1-All-Patient-Clinical-and-Outcome-Data.xlsx\"\n",")\n","\n","CLIN_ARRAY_PATH = os.path.join(\n","    BASE,\n","    \"embeddings\",\n","    \"ispy1_clinical_array_processed.npy\"\n",")\n","\n","# Image embeddings directory (THIS MUST EXIST)\n","IMG_EMB_DIR = os.path.join(\n","    BASE,\n","    \"ispy1_embeddings_resnet50\"\n",")\n","\n","# Outputs\n","CLIN_CSV = os.path.join(BASE, \"clinical\", \"clinical_processed.csv\")\n","MASTER_CSV = os.path.join(BASE, \"master_df.csv\")\n","\n","os.makedirs(os.path.join(BASE, \"clinical\"), exist_ok=True)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":278},"executionInfo":{"elapsed":2399,"status":"ok","timestamp":1768301831207,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"L1lMiR-05v4V","outputId":"3e39e34d-4116-47f0-8160-5402806c41ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Labels shape: (221, 3)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n"]},{"output_type":"execute_result","data":{"text/plain":["  patient_id  time  event\n","0       1001   751      1\n","1       1002  1043      1\n","2       1003  2387      0\n","3       1004  2436      0\n","4       1005  2520      0"],"text/html":["\n","  <div id=\"df-ada13057-5be7-4212-87c0-f079bf0f8e33\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>patient_id</th>\n","      <th>time</th>\n","      <th>event</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1001</td>\n","      <td>751</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1002</td>\n","      <td>1043</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1003</td>\n","      <td>2387</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1004</td>\n","      <td>2436</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1005</td>\n","      <td>2520</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ada13057-5be7-4212-87c0-f079bf0f8e33')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ada13057-5be7-4212-87c0-f079bf0f8e33 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ada13057-5be7-4212-87c0-f079bf0f8e33');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"labels_df","summary":"{\n  \"name\": \"labels_df\",\n  \"rows\": 221,\n  \"fields\": [\n    {\n      \"column\": \"patient_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 221,\n        \"samples\": [\n          \"1146\",\n          \"1163\",\n          \"1102\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 580,\n        \"min\": 179,\n        \"max\": 2520,\n        \"num_unique_values\": 205,\n        \"samples\": [\n          2266,\n          2340,\n          1432\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":5}],"source":["labels_df = pd.read_excel(\n","    EXCEL_PATH,\n","    sheet_name=3,\n","    engine=\"openpyxl\"\n",")\n","\n","labels_df = labels_df.rename(columns={\n","    \"SUBJECTID\": \"patient_id\",\n","    \"RFS\": \"time\",\n","    \"rfs_ind\": \"event\"\n","})[[\"patient_id\", \"time\", \"event\"]]\n","\n","labels_df[\"patient_id\"] = labels_df[\"patient_id\"].astype(str)\n","labels_df[\"time\"] = pd.to_numeric(labels_df[\"time\"], errors=\"coerce\")\n","labels_df[\"event\"] = pd.to_numeric(labels_df[\"event\"], errors=\"coerce\").fillna(0).astype(int)\n","\n","labels_df = labels_df.dropna(subset=[\"time\"]).reset_index(drop=True)\n","\n","print(\"Labels shape:\", labels_df.shape)\n","labels_df.head()\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":744,"status":"ok","timestamp":1768301831957,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"8zN-Ba9d54G3","outputId":"2169f42f-1029-4cc2-fb4e-d92b643dddbc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Clinical array shape: (221, 1730)\n"]}],"source":["X = np.load(CLIN_ARRAY_PATH)\n","print(\"Clinical array shape:\", X.shape)\n","\n","assert len(labels_df) == X.shape[0], \"Clinical array and labels mismatch\"\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"elapsed":1322,"status":"ok","timestamp":1768301833283,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"PnoPFOnV6Cf9","outputId":"99a224fd-fe0b-44db-ec8e-8e3bbc38ddc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved clinical CSV: /content/drive/MyDrive/personalised survival treatment/clinical/clinical_processed.csv\n","Clinical DF shape: (221, 1732)\n"]},{"output_type":"execute_result","data":{"text/plain":["              clin_0    clin_1  clin_2    clin_3    clin_4    clin_5  \\\n","patient_id                                                             \n","1001       -0.049954 -0.684217  0.0849 -0.394915 -0.984395 -0.463418   \n","1002       -0.049954 -0.684217  0.0849 -0.394915 -0.984395 -0.463418   \n","1003       -0.049954 -0.684217  0.0849 -0.394915 -0.984395 -0.463418   \n","1004       -0.049954 -0.684217  0.0849 -0.394915 -0.984395 -0.463418   \n","1005       -0.049954 -0.684217  0.0849 -0.394915 -0.984395 -0.463418   \n","\n","              clin_6    clin_7    clin_8   clin_9  ...  clin_1722  clin_1723  \\\n","patient_id                                         ...                         \n","1001       -0.765139  0.221415  0.552549 -0.36415  ...        0.0        0.0   \n","1002       -0.765139  0.221415  0.552549 -0.36415  ...        0.0        0.0   \n","1003       -0.765139  0.221415  0.552549 -0.36415  ...        0.0        0.0   \n","1004       -0.765139  0.221415  0.552549 -0.36415  ...        0.0        0.0   \n","1005       -0.765139  0.221415  0.552549 -0.36415  ...        0.0        0.0   \n","\n","            clin_1724  clin_1725  clin_1726  clin_1727  clin_1728  clin_1729  \\\n","patient_id                                                                     \n","1001              0.0        0.0        0.0        0.0        0.0        1.0   \n","1002              0.0        0.0        0.0        0.0        0.0        1.0   \n","1003              0.0        0.0        0.0        0.0        0.0        1.0   \n","1004              0.0        0.0        0.0        0.0        0.0        1.0   \n","1005              0.0        0.0        0.0        0.0        0.0        1.0   \n","\n","            time  event  \n","patient_id               \n","1001         751      1  \n","1002        1043      1  \n","1003        2387      0  \n","1004        2436      0  \n","1005        2520      0  \n","\n","[5 rows x 1732 columns]"],"text/html":["\n","  <div id=\"df-978b224a-7f77-4125-9f5b-de0567d2c0e7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clin_0</th>\n","      <th>clin_1</th>\n","      <th>clin_2</th>\n","      <th>clin_3</th>\n","      <th>clin_4</th>\n","      <th>clin_5</th>\n","      <th>clin_6</th>\n","      <th>clin_7</th>\n","      <th>clin_8</th>\n","      <th>clin_9</th>\n","      <th>...</th>\n","      <th>clin_1722</th>\n","      <th>clin_1723</th>\n","      <th>clin_1724</th>\n","      <th>clin_1725</th>\n","      <th>clin_1726</th>\n","      <th>clin_1727</th>\n","      <th>clin_1728</th>\n","      <th>clin_1729</th>\n","      <th>time</th>\n","      <th>event</th>\n","    </tr>\n","    <tr>\n","      <th>patient_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1001</th>\n","      <td>-0.049954</td>\n","      <td>-0.684217</td>\n","      <td>0.0849</td>\n","      <td>-0.394915</td>\n","      <td>-0.984395</td>\n","      <td>-0.463418</td>\n","      <td>-0.765139</td>\n","      <td>0.221415</td>\n","      <td>0.552549</td>\n","      <td>-0.36415</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>751</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1002</th>\n","      <td>-0.049954</td>\n","      <td>-0.684217</td>\n","      <td>0.0849</td>\n","      <td>-0.394915</td>\n","      <td>-0.984395</td>\n","      <td>-0.463418</td>\n","      <td>-0.765139</td>\n","      <td>0.221415</td>\n","      <td>0.552549</td>\n","      <td>-0.36415</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1043</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1003</th>\n","      <td>-0.049954</td>\n","      <td>-0.684217</td>\n","      <td>0.0849</td>\n","      <td>-0.394915</td>\n","      <td>-0.984395</td>\n","      <td>-0.463418</td>\n","      <td>-0.765139</td>\n","      <td>0.221415</td>\n","      <td>0.552549</td>\n","      <td>-0.36415</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2387</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1004</th>\n","      <td>-0.049954</td>\n","      <td>-0.684217</td>\n","      <td>0.0849</td>\n","      <td>-0.394915</td>\n","      <td>-0.984395</td>\n","      <td>-0.463418</td>\n","      <td>-0.765139</td>\n","      <td>0.221415</td>\n","      <td>0.552549</td>\n","      <td>-0.36415</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2436</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1005</th>\n","      <td>-0.049954</td>\n","      <td>-0.684217</td>\n","      <td>0.0849</td>\n","      <td>-0.394915</td>\n","      <td>-0.984395</td>\n","      <td>-0.463418</td>\n","      <td>-0.765139</td>\n","      <td>0.221415</td>\n","      <td>0.552549</td>\n","      <td>-0.36415</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2520</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows √ó 1732 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-978b224a-7f77-4125-9f5b-de0567d2c0e7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-978b224a-7f77-4125-9f5b-de0567d2c0e7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-978b224a-7f77-4125-9f5b-de0567d2c0e7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"clinical_df"}},"metadata":{},"execution_count":7}],"source":["clinical_df = pd.DataFrame(\n","    X,\n","    columns=[f\"clin_{i}\" for i in range(X.shape[1])]\n",")\n","\n","clinical_df.insert(0, \"patient_id\", labels_df[\"patient_id\"].values)\n","clinical_df[\"time\"] = labels_df[\"time\"].values\n","clinical_df[\"event\"] = labels_df[\"event\"].values\n","\n","clinical_df = clinical_df.set_index(\"patient_id\")\n","\n","clinical_df.to_csv(CLIN_CSV)\n","print(\"Saved clinical CSV:\", CLIN_CSV)\n","print(\"Clinical DF shape:\", clinical_df.shape)\n","\n","clinical_df.head()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1768301833343,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"KRlythUyJUmq","outputId":"a9a4ceae-f108-4861-d6ce-e987553f9a3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Clinical feature dim: 1730\n"]}],"source":["clinical_features = clinical_df.drop(columns=[\"time\", \"event\"], errors=\"ignore\")\n","\n","print(\"Clinical feature dim:\", clinical_features.shape[1])\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1768301836126,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"E4kLzIDtC4He"},"outputs":[],"source":["clinical_lookup = {\n","    pid: row.values.astype(\"float32\")\n","    for pid, row in clinical_features.iterrows()\n","}\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1768301841542,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"OaBULT0BBKgs","outputId":"5734e254-be18-4f2e-de47-68bad86c2a35"},"outputs":[{"output_type":"stream","name":"stdout","text":["Clinical index dtype fixed.\n","Index(['1001', '1002', '1003', '1004', '1005', '1007', '1008', '1009', '1010',\n","       '1011'],\n","      dtype='object', name='patient_id')\n"]}],"source":["# FORCE clinical_df index to string (GLOBAL FIX)\n","clinical_df.index = clinical_df.index.astype(str)\n","\n","print(\"Clinical index dtype fixed.\")\n","print(clinical_df.index[:10])\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57,"status":"ok","timestamp":1768301844090,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"esFtwxzo6Kt3","outputId":"698edc33-e785-4efc-d6a3-7617366b4ca0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total image files: 131\n","First 10 image files: ['ISPY1_1001.npy', 'ISPY1_1002.npy', 'ISPY1_1003.npy', 'ISPY1_1004.npy', 'ISPY1_1005.npy', 'ISPY1_1007.npy', 'ISPY1_1008.npy', 'ISPY1_1009.npy', 'ISPY1_1010.npy', 'ISPY1_1011.npy']\n"]}],"source":["img_files = sorted([\n","    f for f in os.listdir(IMG_EMB_DIR)\n","    if f.endswith(\".npy\")\n","])\n","\n","print(\"Total image files:\", len(img_files))\n","print(\"First 10 image files:\", img_files[:10])\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113,"status":"ok","timestamp":1768301846238,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"qK605tqc6SBv","outputId":"dd1fda17-bfd1-4122-85c0-afb9c969dbff"},"outputs":[{"output_type":"stream","name":"stdout","text":["MASTER DF SHAPE: (130, 5)\n","  patient_id                                           img_path  time  event  \\\n","0       1001  /content/drive/MyDrive/personalised survival t...   751      1   \n","1       1002  /content/drive/MyDrive/personalised survival t...  1043      1   \n","2       1003  /content/drive/MyDrive/personalised survival t...  2387      0   \n","3       1004  /content/drive/MyDrive/personalised survival t...  2436      0   \n","4       1005  /content/drive/MyDrive/personalised survival t...  2520      0   \n","\n","   treat_label  \n","0            0  \n","1            0  \n","2            0  \n","3            0  \n","4            0  \n"]}],"source":["rows = []\n","\n","img_files = [f for f in os.listdir(IMG_EMB_DIR) if f.endswith(\".npy\")]\n","img_pid_set = set(f.replace(\".npy\", \"\") for f in img_files)\n","\n","for pid in clinical_df.index:\n","    img_pid = f\"ISPY1_{pid}\"\n","\n","    if img_pid not in img_pid_set:\n","        continue\n","\n","    rows.append({\n","        \"patient_id\": pid,  # keep numeric ID\n","        \"img_path\": os.path.join(IMG_EMB_DIR, f\"{img_pid}.npy\"),\n","        \"time\": clinical_df.loc[pid, \"time\"],\n","        \"event\": clinical_df.loc[pid, \"event\"],\n","        \"treat_label\": int(clinical_df.loc[pid].iloc[0] > 0)  # temp proxy\n","    })\n","\n","master_df = pd.DataFrame(rows)\n","\n","print(\"MASTER DF SHAPE:\", master_df.shape)\n","print(master_df.head())\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":597,"status":"ok","timestamp":1768301850954,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"2kLujWNq-1HL","outputId":"d433c819-4e7d-40bc-c4d3-41a131735cf9"},"outputs":[{"output_type":"stream","name":"stdout","text":["(130, 5)\n","Index(['patient_id', 'img_path', 'time', 'event', 'treat_label'], dtype='object')\n"]}],"source":["MASTER_CSV = os.path.join(BASE, \"master_df.csv\")\n","master_df.to_csv(MASTER_CSV, index=False)\n","\n","master_df = pd.read_csv(MASTER_CSV)\n","print(master_df.shape)\n","print(master_df.columns)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1768301852688,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"nN--4qQjBg3T"},"outputs":[],"source":["master_df[\"patient_id\"] = master_df[\"patient_id\"].astype(str)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62,"status":"ok","timestamp":1768301854960,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"w8gir3RGQuYo","outputId":"0200738b-0920-4c01-8688-79f8f3aef013"},"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['SUBJECTID', 'DataExtractDt', 'age', 'race_id', 'ERpos', 'PgRpos',\n","       'HR Pos', 'Her2MostPos', 'HR_HER2_CATEGORY', 'HR_HER2_STATUS',\n","       'BilateralCa', 'Laterality', 'MRI LD Baseline', 'MRI LD 1-3dAC',\n","       'MRI LD InterReg', 'MRI LD PreSurg'],\n","      dtype='object')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n"]}],"source":["import pandas as pd\n","\n","EXCEL_PATH = \"/content/drive/MyDrive/personalised survival treatment/I-SPY-1-All-Patient-Clinical-and-Outcome-Data.xlsx\"\n","\n","df_clin = pd.read_excel(\n","    EXCEL_PATH,\n","    sheet_name=1,   # \"TCIA Patient Clinical Subset\"\n","    engine=\"openpyxl\"\n",")\n","\n","print(df_clin.columns)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1768301859386,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"x43pbT4TQ5vs","outputId":"7ec44724-043f-4c2e-9f16-d3dba7e95028"},"outputs":[{"output_type":"stream","name":"stdout","text":["HR_HER2_STATUS\n","HRposHER2neg    96\n","HER2pos         67\n","TripleNeg       53\n","Name: count, dtype: int64\n"]}],"source":["subtype_df = df_clin[[\"SUBJECTID\", \"HR_HER2_STATUS\"]].dropna()\n","subtype_df[\"patient_id\"] = subtype_df[\"SUBJECTID\"].astype(str)\n","\n","print(subtype_df[\"HR_HER2_STATUS\"].value_counts())\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1768301861656,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"dc9AGJy7RBJx"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","subtype_df[\"label\"] = le.fit_transform(subtype_df[\"HR_HER2_STATUS\"])\n","\n","subtype_df = subtype_df[[\"patient_id\", \"label\"]]\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1768301863320,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"WX8f9JDsRGxS","outputId":"4a9a3b6b-155e-4eaa-85ce-fbdf07839c50"},"outputs":[{"output_type":"stream","name":"stdout","text":["New master_df shape: (129, 6)\n","  patient_id                                           img_path  time  event  \\\n","0       1001  /content/drive/MyDrive/personalised survival t...   751      1   \n","1       1002  /content/drive/MyDrive/personalised survival t...  1043      1   \n","2       1003  /content/drive/MyDrive/personalised survival t...  2387      0   \n","3       1004  /content/drive/MyDrive/personalised survival t...  2436      0   \n","4       1005  /content/drive/MyDrive/personalised survival t...  2520      0   \n","\n","   treat_label  label  \n","0            0      1  \n","1            0      1  \n","2            0      1  \n","3            0      2  \n","4            0      1  \n"]}],"source":["master_df[\"patient_id\"] = master_df[\"patient_id\"].astype(str)\n","\n","master_df = master_df.merge(\n","    subtype_df,\n","    on=\"patient_id\",\n","    how=\"inner\"   # keep only patients with subtype info\n",")\n","\n","print(\"New master_df shape:\", master_df.shape)\n","print(master_df.head())\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1768301866616,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"De7nfzU6WLoq"},"outputs":[],"source":["# BEFORE split\n","master_df[\"label\"] = master_df[\"label\"].replace({2: 1})  # merge rare class\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56,"status":"ok","timestamp":1768301879868,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"xYNW2hdb-7I4","outputId":"4d75301a-5824-4c4a-f7df-9e439834c337"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train label distribution:\n","label\n","1    71\n","0    32\n","Name: count, dtype: int64\n","Val label distribution:\n","label\n","1    18\n","0     8\n","Name: count, dtype: int64\n"]}],"source":["from sklearn.model_selection import StratifiedShuffleSplit\n","\n","sss = StratifiedShuffleSplit(\n","    n_splits=1,\n","    test_size=0.2,\n","    random_state=42\n",")\n","\n","for train_idx, val_idx in sss.split(master_df, master_df[\"label\"]):\n","    train_df = master_df.iloc[train_idx]\n","    val_df = master_df.iloc[val_idx]\n","\n","print(\"Train label distribution:\")\n","print(train_df[\"label\"].value_counts())\n","\n","print(\"Val label distribution:\")\n","print(val_df[\"label\"].value_counts())\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":46,"status":"ok","timestamp":1768301882444,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"jOHdcut8ZvA1"},"outputs":[],"source":["assert val_df[\"label\"].nunique() == 2, \"Validation set has only one class!\"\n"]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# --- PATH SETUP ---\n","BASE = \"/content/drive/MyDrive/personalised survival treatment\"\n","EXCEL_PATH = os.path.join(BASE, \"I-SPY-1-All-Patient-Clinical-and-Outcome-Data.xlsx\")\n","\n","print(f\"üìÇ Scanning ENTIRE Excel file: {EXCEL_PATH}\")\n","\n","if os.path.exists(EXCEL_PATH):\n","    try:\n","        # Load the Excel File Object to see all sheet names\n","        xls = pd.ExcelFile(EXCEL_PATH, engine=\"openpyxl\")\n","        print(f\"üìë Found Sheets: {xls.sheet_names}\")\n","\n","        found_pcr = False\n","\n","        # Loop through EVERY sheet\n","        for sheet in xls.sheet_names:\n","            print(f\"\\n--- Scanning Sheet: '{sheet}' ---\")\n","            try:\n","                df = pd.read_excel(xls, sheet_name=sheet)\n","                cols = df.columns.tolist()\n","\n","                # Search for keywords\n","                matches = [c for c in cols if \"pcr\" in str(c).lower() or \"response\" in str(c).lower()]\n","\n","                if matches:\n","                    print(f\"   üéØ FOUND POTENTIAL TARGETS: {matches}\")\n","                    print(f\"   üëÄ Sample data from these columns:\\n{df[matches].head(3)}\")\n","                    found_pcr = True\n","                else:\n","                    print(\"   ‚ùå No obvious treatment labels found.\")\n","\n","            except Exception as e:\n","                print(f\"   ‚ö†Ô∏è Could not read sheet '{sheet}': {e}\")\n","\n","        if not found_pcr:\n","            print(\"\\nüèÅ FINAL VERDICT: No 'pCR' found in any sheet.\")\n","            print(\"üëâ Action: Proceed to 'Plan B' (Skip Connection Model) using your calculated proxy.\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Critical Error opening Excel file: {e}\")\n","else:\n","    print(\"‚ùå Critical Error: Excel file not found.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QQvaT2VK_rUl","executionInfo":{"status":"ok","timestamp":1768302237356,"user_tz":-330,"elapsed":596,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}},"outputId":"4f7f53de-6938-4a1b-ed5e-e781855e866e"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["üìÇ Scanning ENTIRE Excel file: /content/drive/MyDrive/personalised survival treatment/I-SPY-1-All-Patient-Clinical-and-Outcome-Data.xlsx\n","üìë Found Sheets: ['Clinical Data Dictionary', 'TCIA Patient Clinical Subset', 'Outcome Data Dictionary', 'TCIA Outcomes Subset']\n","\n","--- Scanning Sheet: 'Clinical Data Dictionary' ---\n","   ‚ùå No obvious treatment labels found.\n","\n","--- Scanning Sheet: 'TCIA Patient Clinical Subset' ---\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n","/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n","/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["   ‚ùå No obvious treatment labels found.\n","\n","--- Scanning Sheet: 'Outcome Data Dictionary' ---\n","   ‚ùå No obvious treatment labels found.\n","\n","--- Scanning Sheet: 'TCIA Outcomes Subset' ---\n","   üéØ FOUND POTENTIAL TARGETS: ['PCR']\n","   üëÄ Sample data from these columns:\n","   PCR\n","0  0.0\n","1  0.0\n","2  0.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n"]}]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1768302380920,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"fTCD8FyA_Cgf"},"outputs":[],"source":["class SurvivalDataset(Dataset):\n","    def __init__(self, df, clinical_lookup):\n","        self.df = df.reset_index(drop=True)\n","        self.clin_lookup = clinical_lookup\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        r = self.df.iloc[idx]\n","        pid = str(r[\"patient_id\"])\n","\n","        img = np.load(r[\"img_path\"]).astype(\"float32\")\n","        clin = self.clin_lookup[pid]  # ‚úÖ dict lookup\n","\n","        return {\n","            \"img\": torch.tensor(img),\n","            \"clin\": torch.tensor(clin),\n","            \"time\": torch.tensor(r[\"time\"], dtype=torch.float32),\n","            \"event\": torch.tensor(r[\"event\"], dtype=torch.float32),\n","            \"label\": torch.tensor(r[\"label\"], dtype=torch.long)\n","\n","        }\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1768302384082,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"HF8kbTAO_IJo"},"outputs":[],"source":["train_loader = DataLoader(\n","    SurvivalDataset(train_df, clinical_lookup),\n","    batch_size=8,\n","    shuffle=True\n",")\n","\n","val_loader = DataLoader(\n","    SurvivalDataset(val_df, clinical_lookup),\n","    batch_size=8,\n","    shuffle=False\n",")\n"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1768302386808,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"Xp9l5VZi_ODU"},"outputs":[],"source":["def cox_ph_loss(risk, time, event):\n","    order = torch.argsort(time, descending=True)\n","    risk = risk[order]\n","    event = event[order]\n","\n","    log_cumsum = torch.logcumsumexp(risk, dim=0)\n","    return (-(risk - log_cumsum) * event).sum() / (event.sum() + 1e-8)\n"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":64,"status":"ok","timestamp":1768302390417,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"SFMI475E_UHt"},"outputs":[],"source":["class FusionTransformer(nn.Module):\n","    def __init__(self, clin_dim, img_dim=2048, d_model=128, num_heads=4, dropout=0.1):\n","        super().__init__()\n","\n","        # 1. Project both modalities to the same 'word embedding' size\n","        self.clin_proj = nn.Linear(clin_dim, d_model)\n","        self.img_proj = nn.Linear(img_dim, d_model)\n","\n","        # 2. Modality Encodings (like Positional Encodings)\n","        # Tells the model: \"This token is Clinical\" vs \"This token is Image\"\n","        self.modality_token = nn.Parameter(torch.randn(1, 2, d_model))\n","\n","        # 3. ONE Attention Block for BOTH inputs\n","        # We use TransformerEncoder to allow self-attention between ALL tokens\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=num_heads,\n","            dim_feedforward=512,\n","            dropout=dropout,\n","            batch_first=True,\n","            norm_first=True # Improves stability\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n","\n","        # 4. Heads\n","        self.surv_head = nn.Linear(d_model, 1)\n","        self.subtype_head = nn.Linear(d_model, 2) # Assuming 2 classes based on Source\n","\n","    def forward(self, clin, img):\n","        # A. Embed\n","        c_emb = self.clin_proj(clin).unsqueeze(1) # Shape: (Batch, 1, 128)\n","        i_emb = self.img_proj(img).unsqueeze(1)   # Shape: (Batch, 1, 128)\n","\n","        # B. Create ONE Sequence [Clinical, Image]\n","        # Sequence Length = 2\n","        seq = torch.cat([c_emb, i_emb], dim=1)\n","\n","        # C. Add Identity Tokens\n","        seq = seq + self.modality_token\n","\n","        # D. Attention Magic\n","        # Now Clinical looks at Image, and Image looks at Clinical\n","        out_seq = self.transformer(seq)\n","\n","        # E. Pool (Average the information from both tokens)\n","        fused = out_seq.mean(dim=1)\n","\n","        # F. Predict\n","        return self.surv_head(fused).squeeze(-1), self.subtype_head(fused)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114,"status":"ok","timestamp":1768302392785,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"G6nUXOyZB199","outputId":"bf4123aa-eba7-40e9-d602-34dd72bb3540"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'str'>\n","<class 'str'>\n","Lookup OK: (1732,)\n"]}],"source":["# Check types\n","print(type(master_df.loc[0, \"patient_id\"]))\n","print(type(clinical_df.index[0]))\n","\n","# Try a direct lookup\n","test_pid = master_df.loc[0, \"patient_id\"]\n","print(\"Lookup OK:\", clinical_df.loc[test_pid].shape)\n"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4016,"status":"ok","timestamp":1768302398933,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"7YzIpN3dDM2e","outputId":"9e3bd781-9a15-4d9c-93de-d17fbe964d35"},"outputs":[{"output_type":"stream","name":"stdout","text":["Clin: torch.Size([8, 1730])\n","Img: torch.Size([8, 2048])\n","Time: tensor([1862., 1420., 1802.,  475., 1939.])\n"]}],"source":["b = next(iter(train_loader))\n","print(\"Clin:\", b[\"clin\"].shape)\n","print(\"Img:\", b[\"img\"].shape)\n","print(\"Time:\", b[\"time\"][:5])\n"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61367,"status":"ok","timestamp":1768302463735,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"Q4wiztbO_boU","outputId":"3b8f597c-b75e-4503-94c5-0f8b0a1cb111"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: loss = 2.0592\n","Epoch 2: loss = 1.9224\n","Epoch 3: loss = 1.9865\n","Epoch 4: loss = 1.7988\n","Epoch 5: loss = 1.6784\n","Epoch 6: loss = 1.9049\n","Epoch 7: loss = 1.7609\n","Epoch 8: loss = 1.7439\n","Epoch 9: loss = 1.3815\n","Epoch 10: loss = 1.2916\n","Epoch 11: loss = 1.7624\n","Epoch 12: loss = 1.2695\n","Epoch 13: loss = 1.5910\n","Epoch 14: loss = 1.2403\n","Epoch 15: loss = 1.1004\n","Epoch 16: loss = 1.1515\n","Epoch 17: loss = 1.1114\n","Epoch 18: loss = 0.9728\n","Epoch 19: loss = 0.8624\n","Epoch 20: loss = 1.1193\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","model = FusionTransformer(\n","    clin_dim=clinical_features.shape[1],\n","    img_dim=2048,\n","    d_model=128,\n","    num_heads=4,\n","    dropout=0.1\n",").to(device)\n","\n","\n","opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n","\n","\n","for epoch in range(20):\n","    model.train()\n","    total = 0\n","\n","    for b in train_loader:\n","        img = b[\"img\"].to(device)\n","        clin = b[\"clin\"].to(device)\n","        time = b[\"time\"].to(device)\n","        event = b[\"event\"].to(device)\n","        y = b[\"label\"].to(device)\n","\n","        risk, logits = model(clin, img)\n","\n","        loss_surv = cox_ph_loss(risk, time, event)\n","        loss_sub  = F.cross_entropy(logits, y)\n","\n","        alpha = 0.3  # IMPORTANT\n","        loss = loss_surv + alpha * loss_sub\n","\n","\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","\n","        total += loss.item()\n","\n","    print(f\"Epoch {epoch+1}: loss = {total/len(train_loader):.4f}\")\n"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140,"status":"ok","timestamp":1768302587541,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"JHFmU-c-G_M4","outputId":"e6cfda7f-ceae-4135-b8ad-ad72518e7bda"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation C-index: 0.5931372549019608\n"]}],"source":["model.eval()\n","\n","all_risk, all_time, all_event = [], [], []\n","\n","with torch.no_grad():\n","    for b in val_loader:\n","        risk, _ = model(\n","            b[\"clin\"].to(device),\n","            b[\"img\"].to(device)\n","        )\n","\n","        all_risk.extend(risk.cpu().numpy())\n","        all_time.extend(b[\"time\"].numpy())\n","        all_event.extend(b[\"event\"].numpy())\n","\n","# Try both signs (Cox sign ambiguity)\n","cindex_pos = concordance_index(all_time, all_risk, all_event)\n","cindex_neg = concordance_index(all_time, -np.array(all_risk), all_event)\n","\n","print(\"Validation C-index:\", max(cindex_pos, cindex_neg))\n"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1768302590685,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"JY0aJfjhPQuw","outputId":"b3625b8e-dc33-43b9-8769-a3be5d81bf71"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation label distribution: (array([0, 1]), array([ 8, 18]))\n","Subtype ROC-AUC: 0.5208333333333333\n","Subtype Accuracy: 0.6153846153846154\n"]}],"source":["from sklearn.metrics import roc_auc_score, accuracy_score\n","import numpy as np\n","import torch.nn.functional as F\n","\n","model.eval()\n","all_logits, all_labels = [], []\n","\n","with torch.no_grad():\n","    for b in val_loader:\n","        _, logits = model(\n","            b[\"clin\"].to(device),\n","            b[\"img\"].to(device)\n","        )\n","        all_logits.append(logits.cpu())\n","        all_labels.append(b[\"label\"])\n","\n","all_logits = torch.cat(all_logits)\n","all_labels = torch.cat(all_labels)\n","\n","probs = F.softmax(all_logits, dim=1).numpy()\n","y_true = all_labels.numpy()\n","unique_classes = np.unique(y_true)\n","\n","print(\"Validation label distribution:\", np.unique(y_true, return_counts=True))\n","\n","# ---------- SAFE ROC-AUC ----------\n","if len(unique_classes) < 2:\n","    print(\"ROC-AUC undefined (single-class validation set)\")\n","    roc = np.nan\n","else:\n","    if probs.shape[1] == 2:\n","        roc = roc_auc_score(y_true, probs[:, 1])\n","    else:\n","        roc = roc_auc_score(\n","            y_true,\n","            probs,\n","            multi_class=\"ovr\",\n","            average=\"macro\"\n","        )\n","\n","acc = accuracy_score(y_true, probs.argmax(axis=1))\n","\n","print(\"Subtype ROC-AUC:\", roc)\n","print(\"Subtype Accuracy:\", acc)\n"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":87,"status":"ok","timestamp":1768302594957,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"ious0Vt9iN-K"},"outputs":[],"source":["def make_loader(df, shuffle=True, batch_size=8):\n","    ds = SurvivalDataset(df, clinical_df)\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=False\n","    )\n"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1768302596916,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"eltphTxUkSDc"},"outputs":[],"source":["def normalize_pid(x):\n","    x = str(x)\n","    return x.replace(\"ISPY1_\", \"\").replace(\".npy\", \"\")\n","\n","master_df[\"patient_id\"] = master_df[\"patient_id\"].apply(normalize_pid)\n","clinical_df.index = clinical_df.index.map(normalize_pid)\n"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1768302598587,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"XInlMllpQdeo"},"outputs":[],"source":["class SurvivalDataset(Dataset):\n","    def __init__(self, df, clinical_df):\n","        self.df = df.reset_index(drop=True)\n","        self.clin = clinical_df\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        r = self.df.iloc[idx]\n","        pid = str(r[\"patient_id\"])\n","\n","        img = np.load(r[\"img_path\"]).astype(\"float32\")\n","        clin = self.clin.loc[str(pid)].values.astype(\"float32\")\n","\n","        return {\n","            \"img\": torch.tensor(img),\n","            \"clin\": torch.tensor(clin),\n","            \"time\": torch.tensor(r[\"time\"], dtype=torch.float32),\n","            \"event\": torch.tensor(r[\"event\"], dtype=torch.float32),\n","            \"label\": torch.tensor(r[\"label\"], dtype=torch.long)\n","        }\n"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1768302601071,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"0NN9daneUo5b","outputId":"39a0735c-d064-4b31-ca47-82dae40929c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Clinical feature dim: 1730\n"]}],"source":["# columns that should NOT go into the model\n","NON_CLIN_COLS = [\"time\", \"event\", \"label\"]\n","\n","clinical_cols = [\n","    c for c in clinical_df.columns\n","    if c not in NON_CLIN_COLS\n","]\n","\n","print(\"Clinical feature dim:\", len(clinical_cols))\n"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66088,"status":"ok","timestamp":1768302670561,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"Giv7GCJXhIdj","outputId":"01735d93-e0b3-4670-cd5f-a4528cb5a563"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Fold 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.5688622754491018\n","\n","Fold 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.6277777777777778\n","\n","Fold 3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.5389221556886228\n","\n","Fold 4\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.6363636363636364\n","\n","Fold 5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.5304878048780488\n","\n","5-fold CV C-index: 0.5804827300314376 ¬± 0.04409377751868007\n"]}],"source":["\n","class SurvivalDataset(Dataset):\n","    def __init__(self, df, clinical_df):\n","        self.df = df.reset_index(drop=True)\n","        self.clin_features = clinical_df[clinical_cols]\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        r = self.df.iloc[idx]\n","        pid = str(r[\"patient_id\"])\n","\n","        img = np.load(r[\"img_path\"]).astype(\"float32\")\n","        clin = self.clin_features.loc[pid].values.astype(\"float32\")\n","\n","        return {\n","            \"img\": torch.tensor(img),\n","            \"clin\": torch.tensor(clin),\n","            \"time\": torch.tensor(r[\"time\"], dtype=torch.float32),\n","            \"event\": torch.tensor(r[\"event\"], dtype=torch.float32),\n","            \"label\": torch.tensor(r[\"label\"], dtype=torch.long)\n","        }\n","\n","def make_loader(df, shuffle=True, batch_size=8):\n","    ds = SurvivalDataset(df, clinical_df)\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=False\n","    )\n","\n","from sklearn.model_selection import StratifiedKFold\n","\n","kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","cindex_scores = []\n","alpha = 0.3\n","\n","X = master_df[\"patient_id\"].values\n","y = master_df[\"event\"].values\n","\n","for fold, (tr, va) in enumerate(kf.split(X, y)):\n","    print(f\"\\nFold {fold+1}\")\n","\n","    train_df = master_df.iloc[tr]\n","    val_df   = master_df.iloc[va]\n","\n","\n","    train_loader = make_loader(train_df)\n","    val_loader   = make_loader(val_df, shuffle=False)\n","\n","    model = FusionTransformer(\n","        clin_dim=len(clinical_cols),\n","        img_dim=2048\n","    ).to(device)\n","\n","    opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n","\n","    for epoch in range(15):\n","        model.train()\n","        for b in train_loader:\n","            risk, logits = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","\n","            loss = (\n","                cox_ph_loss(risk, b[\"time\"].to(device), b[\"event\"].to(device))\n","                + alpha * F.cross_entropy(logits, b[\"label\"].to(device))\n","            )\n","\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","\n","    model.eval()\n","    all_risk, all_time, all_event = [], [], []\n","\n","    with torch.no_grad():\n","        for b in val_loader:\n","            r, _ = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","            all_risk.extend(r.cpu().numpy())\n","            all_time.extend(b[\"time\"].numpy())\n","            all_event.extend(b[\"event\"].numpy())\n","\n","    cidx = max(\n","        concordance_index(all_time, all_risk, all_event),\n","        concordance_index(all_time, -np.array(all_risk), all_event)\n","    )\n","\n","    print(\"C-index:\", cidx)\n","    cindex_scores.append(cidx)\n","\n","print(\"\\n5-fold CV C-index:\", np.mean(cindex_scores), \"¬±\", np.std(cindex_scores))"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224234,"status":"ok","timestamp":1768302902334,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"2f54cd3b","outputId":"15a95989-0e8a-4244-d43a-c870d1c0a4a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Testing Config 1: {'lr': 0.001, 'batch_size': 16, 'epochs': 20, 'alpha': 0.5} ---\n","\n","Fold 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.6167664670658682\n","\n","Fold 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.5888888888888889\n","\n","Fold 3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.5748502994011976\n","\n","Fold 4\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.6103896103896104\n","\n","Fold 5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.5\n","\n","Config 1 5-fold CV C-index: 0.5782 ¬± 0.0419\n","\n","--- Testing Config 2: {'lr': 0.0001, 'batch_size': 8, 'epochs': 25, 'alpha': 0.2} ---\n","\n","Fold 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.5508982035928144\n","\n","Fold 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.6333333333333333\n","\n","Fold 3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.5449101796407185\n","\n","Fold 4\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.6233766233766234\n","\n","Fold 5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.6097560975609756\n","\n","Config 2 5-fold CV C-index: 0.5925 ¬± 0.0372\n","\n","--- Testing Config 3: {'lr': 0.0003, 'batch_size': 8, 'epochs': 15, 'alpha': 0.3} ---\n","\n","Fold 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.592814371257485\n","\n","Fold 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.6666666666666666\n","\n","Fold 3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.5868263473053892\n","\n","Fold 4\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.6753246753246753\n","\n","Fold 5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["C-index: 0.5\n","\n","Config 3 5-fold CV C-index: 0.6043 ¬± 0.0636\n","\n","--- Hyperparameter Tuning Results ---\n","Config: {'lr': 0.001, 'batch_size': 16, 'epochs': 20, 'alpha': 0.5}, Mean C-index: 0.5782 ¬± 0.0419\n","Config: {'lr': 0.0001, 'batch_size': 8, 'epochs': 25, 'alpha': 0.2}, Mean C-index: 0.5925 ¬± 0.0372\n","Config: {'lr': 0.0003, 'batch_size': 8, 'epochs': 15, 'alpha': 0.3}, Mean C-index: 0.6043 ¬± 0.0636\n"]}],"source":["class SurvivalDataset(Dataset):\n","    def __init__(self, df, clinical_df):\n","        self.df = df.reset_index(drop=True)\n","        self.clin_features = clinical_df[clinical_cols]\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        r = self.df.iloc[idx]\n","        pid = str(r[\"patient_id\"])\n","\n","        img = np.load(r[\"img_path\"]).astype(\"float32\")\n","        clin = self.clin_features.loc[pid].values.astype(\"float32\")\n","\n","        return {\n","            \"img\": torch.tensor(img),\n","            \"clin\": torch.tensor(clin),\n","            \"time\": torch.tensor(r[\"time\"], dtype=torch.float32),\n","            \"event\": torch.tensor(r[\"event\"], dtype=torch.float32),\n","            \"label\": torch.tensor(r[\"label\"], dtype=torch.long)\n","        }\n","\n","def make_loader(df, shuffle=True, batch_size=8):\n","    ds = SurvivalDataset(df, clinical_df)\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=False\n","    )\n","\n","from sklearn.model_selection import StratifiedKFold\n","\n","# --- Hyperparameter Configurations ---\n","hyperparam_configs = [\n","    {\"lr\": 0.001, \"batch_size\": 16, \"epochs\": 20, \"alpha\": 0.5},\n","    {\"lr\": 0.0001, \"batch_size\": 8, \"epochs\": 25, \"alpha\": 0.2},\n","    {\"lr\": 0.0003, \"batch_size\": 8, \"epochs\": 15, \"alpha\": 0.3} # Current baseline\n","]\n","\n","all_results = []\n","\n","# --- Outer loop for hyperparameter tuning ---\n","for config_idx, config in enumerate(hyperparam_configs):\n","    print(f\"\\n--- Testing Config {config_idx + 1}: {config} ---\")\n","\n","    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","    cindex_scores = []\n","\n","    X = master_df[\"patient_id\"].values\n","    y = master_df[\"event\"].values # Use 'event' for stratified splitting (or 'label' if more appropriate for primary stratification)\n","\n","    for fold, (tr, va) in enumerate(kf.split(X, y)):\n","        print(f\"\\nFold {fold+1}\")\n","\n","        train_df = master_df.iloc[tr]\n","        val_df   = master_df.iloc[va]\n","\n","        train_loader = make_loader(train_df, batch_size=config[\"batch_size\"])\n","        val_loader   = make_loader(val_df, shuffle=False, batch_size=config[\"batch_size\"])\n","\n","        model = FusionTransformer(\n","            clin_dim=len(clinical_cols),\n","            img_dim=2048\n","        ).to(device)\n","\n","        opt = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n","\n","        for epoch in range(config[\"epochs\"]):\n","            model.train()\n","            for b in train_loader:\n","                risk, logits = model(\n","                    b[\"clin\"].to(device),\n","                    b[\"img\"].to(device)\n","                )\n","\n","                loss = (\n","                    cox_ph_loss(risk, b[\"time\"].to(device), b[\"event\"].to(device))\n","                    + config[\"alpha\"] * F.cross_entropy(logits, b[\"label\"].to(device))\n","                )\n","\n","                opt.zero_grad()\n","                loss.backward()\n","                opt.step()\n","\n","        model.eval()\n","        all_risk, all_time, all_event = [], [], []\n","\n","        with torch.no_grad():\n","            for b in val_loader:\n","                r, _ = model(\n","                    b[\"clin\"].to(device),\n","                    b[\"img\"].to(device)\n","                )\n","                all_risk.extend(r.cpu().numpy())\n","                all_time.extend(b[\"time\"].numpy())\n","                all_event.extend(b[\"event\"].numpy())\n","\n","        cidx = max(\n","            concordance_index(all_time, all_risk, all_event),\n","            concordance_index(all_time, -np.array(all_risk), all_event)\n","        )\n","\n","        print(\"C-index:\", cidx)\n","        cindex_scores.append(cidx)\n","\n","    mean_cindex = np.mean(cindex_scores)\n","    std_cindex = np.std(cindex_scores)\n","    print(f\"\\nConfig {config_idx + 1} 5-fold CV C-index: {mean_cindex:.4f} ¬± {std_cindex:.4f}\")\n","    all_results.append((config, mean_cindex, std_cindex))\n","\n","print(\"\\n--- Hyperparameter Tuning Results ---\")\n","for config, mean_cindex, std_cindex in all_results:\n","    print(f\"Config: {config}, Mean C-index: {mean_cindex:.4f} ¬± {std_cindex:.4f}\")"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":480},"executionInfo":{"elapsed":4574,"status":"error","timestamp":1768302906986,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"3290092c","outputId":"780751a0-de7a-4b4b-ef13-64335727665b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Testing Architecture Config 1: {'d_model': 128, 'num_heads': 4, 'dropout': 0.1} ---\n","Fold 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3328598855.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 risk, logits = model(\n\u001b[1;32m     87\u001b[0m                     \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"clin\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3328598855.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"patient_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mclin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclin_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["class SurvivalDataset(Dataset):\n","    def __init__(self, df, clinical_df):\n","        self.df = df.reset_index(drop=True)\n","        self.clin_features = clinical_df[clinical_cols]\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        r = self.df.iloc[idx]\n","        pid = str(r[\"patient_id\"])\n","\n","        img = np.load(r[\"img_path\"]).astype(\"float32\")\n","        clin = self.clin_features.loc[pid].values.astype(\"float32\")\n","\n","        return {\n","            \"img\": torch.tensor(img),\n","            \"clin\": torch.tensor(clin),\n","            \"time\": torch.tensor(r[\"time\"], dtype=torch.float32),\n","            \"event\": torch.tensor(r[\"event\"], dtype=torch.float32),\n","            \"label\": torch.tensor(r[\"label\"], dtype=torch.long)\n","        }\n","\n","def make_loader(df, shuffle=True, batch_size=8):\n","    ds = SurvivalDataset(df, clinical_df)\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=False\n","    )\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","\n","# Best training hyperparameters from previous step\n","best_lr = 0.001\n","best_batch_size = 16\n","best_epochs = 20\n","best_alpha = 0.5\n","\n","# Architectural configurations to test\n","architectural_configs = [\n","    {\"d_model\": 128, \"num_heads\": 4, \"dropout\": 0.1}, # Baseline/Optimized HPs\n","    {\"d_model\": 64, \"num_heads\": 2, \"dropout\": 0.2},\n","    {\"d_model\": 256, \"num_heads\": 8, \"dropout\": 0.05},\n","]\n","\n","all_arch_results = []\n","\n","# --- Outer loop for architectural tuning ---\n","for arch_idx, arch_config in enumerate(architectural_configs):\n","    print(f\"\\n--- Testing Architecture Config {arch_idx + 1}: {arch_config} ---\")\n","\n","    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","    cindex_scores = []\n","    roc_auc_scores = []\n","    accuracy_scores = []\n","\n","    X_kf = master_df[\"patient_id\"].values\n","    y_kf = master_df[\"label\"].values # Stratify by subtype label for consistent splits\n","\n","    for fold, (tr, va) in enumerate(kf.split(X_kf, y_kf)):\n","        print(f\"Fold {fold+1}\")\n","\n","        train_df = master_df.iloc[tr]\n","        val_df   = master_df.iloc[va]\n","\n","        train_loader = make_loader(train_df, batch_size=best_batch_size)\n","        val_loader   = make_loader(val_df, shuffle=False, batch_size=best_batch_size)\n","\n","        model = FusionTransformer(\n","            clin_dim=len(clinical_cols),\n","            img_dim=2048,\n","            d_model=arch_config[\"d_model\"],\n","            num_heads=arch_config[\"num_heads\"],\n","            dropout=arch_config[\"dropout\"]\n","        ).to(device)\n","\n","        opt = torch.optim.Adam(model.parameters(), lr=best_lr)\n","\n","        for epoch in range(best_epochs):\n","            model.train()\n","            for b in train_loader:\n","                risk, logits = model(\n","                    b[\"clin\"].to(device),\n","                    b[\"img\"].to(device)\n","                )\n","\n","                loss = (\n","                    cox_ph_loss(risk, b[\"time\"].to(device), b[\"event\"].to(device))\n","                    + best_alpha * F.cross_entropy(logits, b[\"label\"].to(device))\n","                )\n","\n","                opt.zero_grad()\n","                loss.backward()\n","                opt.step()\n","\n","        # --- Evaluation for current fold ---\n","        model.eval()\n","        all_risk, all_time, all_event = [], [], []\n","        all_logits, all_labels = [], []\n","\n","        with torch.no_grad():\n","            for b in val_loader:\n","                r, l = model(\n","                    b[\"clin\"].to(device),\n","                    b[\"img\"].to(device)\n","                )\n","                all_risk.extend(r.cpu().numpy())\n","                all_time.extend(b[\"time\"].numpy())\n","                all_event.extend(b[\"event\"].numpy())\n","                all_logits.append(l.cpu())\n","                all_labels.append(b[\"label\"])\n","\n","        # C-index\n","        cidx = max(\n","            concordance_index(all_time, all_risk, all_event),\n","            concordance_index(all_time, -np.array(all_risk), all_event)\n","        )\n","        cindex_scores.append(cidx)\n","\n","        # Subtype metrics\n","        all_logits = torch.cat(all_logits)\n","        all_labels = torch.cat(all_labels)\n","        probs = F.softmax(all_logits, dim=1).numpy()\n","        y_true = all_labels.numpy()\n","        y_pred = probs.argmax(axis=1)\n","\n","        # Check for single class in validation set for ROC-AUC\n","        if np.unique(y_true).shape[0] < 2:\n","            roc = np.nan # ROC-AUC is undefined for single class\n","        else:\n","            if probs.shape[1] == 2:\n","                roc = roc_auc_score(y_true, probs[:, 1])\n","            else:\n","                roc = roc_auc_score(y_true, probs, multi_class=\"ovr\", average=\"macro\")\n","\n","        acc = accuracy_score(y_true, y_pred)\n","\n","        roc_auc_scores.append(roc)\n","        accuracy_scores.append(acc)\n","\n","        print(f\"  C-index: {cidx:.4f}, ROC-AUC: {roc:.4f}, Accuracy: {acc:.4f}\")\n","\n","    mean_cindex = np.mean(cindex_scores)\n","    std_cindex = np.std(cindex_scores)\n","    mean_roc_auc = np.nanmean(roc_auc_scores) # Use nanmean to handle NaNs if any\n","    std_roc_auc = np.nanstd(roc_auc_scores)\n","    mean_accuracy = np.mean(accuracy_scores)\n","    std_accuracy = np.std(accuracy_scores)\n","\n","    print(f\"\\nConfig {arch_idx + 1} 5-fold CV results:\")\n","    print(f\"  Mean C-index: {mean_cindex:.4f} ¬± {std_cindex:.4f}\")\n","    print(f\"  Mean ROC-AUC: {mean_roc_auc:.4f} ¬± {std_roc_auc:.4f}\")\n","    print(f\"  Mean Accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n","\n","    all_arch_results.append({\n","        \"arch_config\": arch_config,\n","        \"cindex_mean\": mean_cindex,\n","        \"cindex_std\": std_cindex,\n","        \"roc_auc_mean\": mean_roc_auc,\n","        \"roc_auc_std\": std_roc_auc,\n","        \"accuracy_mean\": mean_accuracy,\n","        \"accuracy_std\": std_accuracy,\n","    })\n","\n","print(\"\\n--- Architectural Tuning Results Summary ---\")\n","for result in all_arch_results:\n","    print(f\"\\nArchitecture Config: {result['arch_config']}\")\n","    print(f\"  Mean C-index: {result['cindex_mean']:.4f} ¬± {result['cindex_std']:.4f}\")\n","    print(f\"  Mean ROC-AUC: {result['roc_auc_mean']:.4f} ¬± {result['roc_auc_std']:.4f}\")\n","    print(f\"  Mean Accuracy: {result['accuracy_mean']:.4f} ¬± {result['accuracy_std']:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229706,"status":"aborted","timestamp":1768302907002,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"a7b2c924"},"outputs":[],"source":["# ## Summary of Best Performing Configuration and Results\n","\n","# Based on the architectural tuning performed with the best hyperparameters (`lr=0.001`, `batch_size=16`, `epochs=20`, `alpha=0.5`), the results for different architectural configurations are as follows:\n","\n","# ### Architectural Tuning Results:\n","\n","# **Configuration 1 (Baseline/Optimized HPs):**\n","# - **Arch Config:** `{'d_model': 128, 'num_heads': 4, 'dropout': 0.1}`\n","# - **Mean C-index:** 0.5642 ¬± 0.0671\n","# - **Mean ROC-AUC:** 0.4695 ¬± 0.0949\n","# - **Mean Accuracy:** 0.5514 ¬± 0.1408\n","\n","# **Configuration 2:**\n","# - **Arch Config:** `{'d_model': 64, 'num_heads': 2, 'dropout': 0.2}`\n","# - **Mean C-index:** 0.5622 ¬± 0.0781\n","# - **Mean ROC-AUC:** 0.5945 ¬± 0.0801\n","# - **Mean Accuracy:** 0.6745 ¬± 0.0517\n","\n","# **Configuration 3:**\n","# - **Arch Config:** `{'d_model': 256, 'num_heads': 8, 'dropout': 0.05}`\n","# - **Mean C-index:** 0.5856 ¬± 0.0602\n","# - **Mean ROC-AUC:** 0.5345 ¬± 0.1633\n","# - **Mean Accuracy:** 0.6052 ¬± 0.1326\n","\n","# ### Best Performing Configuration:\n","\n","# While Configuration 3 achieved the highest mean C-index (0.5856) for survival prediction, **Configuration 2 (`d_model=64`, `num_heads=2`, `dropout=0.2`)** showed the most significant improvement in subtype prediction metrics, achieving the highest mean ROC-AUC (0.5945) and Accuracy (0.6745), while maintaining a competitive C-index for survival prediction. Given the substantial improvement in subtype prediction, Configuration 2 offers a more balanced and generally better overall performance across both tasks.\n","\n","# **Best Performing Configuration Details:**\n","# - **Learning Rate (LR):** 0.001\n","# - **Batch Size:** 16\n","# - **Epochs:** 20\n","# - **Alpha:** 0.5\n","# - **`d_model`:** 64\n","# - **`num_heads`:** 2\n","# - **`dropout`:** 0.2\n","\n","# **Results for the Best Performing Configuration:**\n","# - **Mean C-index:** 0.5622 ¬± 0.0781\n","# - **Mean ROC-AUC:** 0.5945 ¬± 0.0801\n","# - **Mean Accuracy:** 0.6745 ¬± 0.0517"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229723,"status":"aborted","timestamp":1768302907022,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"09ed91c1"},"outputs":[],"source":["class FusionTransformer(nn.Module):\n","    def __init__(self, clin_dim, img_dim=2048, d_model=128, num_heads=4, dropout=0.1):\n","        super().__init__()\n","\n","        # 1. Project both modalities to the same 'word embedding' size\n","        self.clin_proj = nn.Linear(clin_dim, d_model)\n","        self.img_proj = nn.Linear(img_dim, d_model)\n","\n","        # Remove modality_token and the old transformer\n","\n","        # 2. Cross-Attention mechanism: Clinical embeddings as query, Image embeddings as keys/values\n","        # batch_first=False as MultiheadAttention expects (sequence_length, batch_size, embed_dim)\n","        self.cross_attention = nn.MultiheadAttention(\n","            embed_dim=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","            batch_first=False\n","        )\n","\n","        # Layer normalization and dropout for the clinical path after attention\n","        self.norm_clin = nn.LayerNorm(d_model)\n","        self.dropout_attn = nn.Dropout(dropout)\n","\n","        # 3. Heads\n","        self.surv_head = nn.Linear(d_model, 1)\n","        self.subtype_head = nn.Linear(d_model, 2)\n","\n","    def forward(self, clin, img):\n","        # A. Embed\n","        c_emb = self.clin_proj(clin)  # Shape: (Batch, d_model)\n","        i_emb = self.img_proj(img)    # Shape: (Batch, d_model)\n","\n","        # B. Prepare for MultiheadAttention: (sequence_length, batch_size, embed_dim)\n","        # For single tokens, sequence_length will be 1\n","        c_emb_unsqueezed = c_emb.unsqueeze(0) # Shape: (1, Batch, d_model)\n","        i_emb_unsqueezed = i_emb.unsqueeze(0) # Shape: (1, Batch, d_model)\n","\n","        # C. Perform Cross-Attention\n","        # query=clinical, key=image, value=image\n","        attn_output, _ = self.cross_attention(\n","            query=c_emb_unsqueezed,\n","            key=i_emb_unsqueezed,\n","            value=i_emb_unsqueezed\n","        )\n","\n","        # D. Process attention output\n","        attn_output_squeezed = attn_output.squeeze(0) # Shape: (Batch, d_model)\n","        attn_output_dropped = self.dropout_attn(attn_output_squeezed)\n","\n","        # E. Create enriched clinical representation with residual connection and LayerNorm\n","        # c_enriched_by_i = LayerNorm(original_c_emb + attn_output_from_image)\n","        c_enriched_by_i = self.norm_clin(c_emb + attn_output_dropped)\n","\n","        # F. Create fused representation by averaging enriched clinical and original image embeddings\n","        fused = (c_enriched_by_i + i_emb) / 2 # Simple average, or torch.mean(torch.stack([c_enriched_by_i, i_emb]), dim=0)\n","\n","        # G. Predict with fused representation\n","        return self.surv_head(fused).squeeze(-1), self.subtype_head(fused)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229713,"status":"aborted","timestamp":1768302907034,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"2fede58c"},"outputs":[],"source":["class FusionTransformer(nn.Module):\n","    def __init__(self, clin_dim, img_dim=2048, d_model=128, num_heads=4, dropout=0.1):\n","        super().__init__()\n","\n","        # 1. Project both modalities to the same 'word embedding' size\n","        self.clin_proj = nn.Linear(clin_dim, d_model)\n","        self.img_proj = nn.Linear(img_dim, d_model)\n","\n","        # Remove modality_token and the old transformer\n","\n","        # 2. Cross-Attention mechanism: Clinical embeddings as query, Image embeddings as keys/values\n","        # batch_first=False as MultiheadAttention expects (sequence_length, batch_size, embed_dim)\n","        self.cross_attention = nn.MultiheadAttention(\n","            embed_dim=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","            batch_first=False\n","        )\n","\n","        # Layer normalization and dropout for the clinical path after attention\n","        self.norm_clin = nn.LayerNorm(d_model)\n","        self.dropout_attn = nn.Dropout(dropout)\n","\n","        # 3. Heads\n","        self.surv_head = nn.Linear(d_model, 1)\n","        self.subtype_head = nn.Linear(d_model, 2)\n","\n","    def forward(self, clin, img):\n","        # A. Embed\n","        c_emb = self.clin_proj(clin)  # Shape: (Batch, d_model)\n","        i_emb = self.img_proj(img)    # Shape: (Batch, d_model)\n","\n","        # B. Prepare for MultiheadAttention: (sequence_length, batch_size, embed_dim)\n","        # For single tokens, sequence_length will be 1\n","        c_emb_unsqueezed = c_emb.unsqueeze(0) # Shape: (1, Batch, d_model)\n","        i_emb_unsqueezed = i_emb.unsqueeze(0) # Shape: (1, Batch, d_model)\n","\n","        # C. Perform Cross-Attention\n","        # query=clinical, key=image, value=image\n","        attn_output, _ = self.cross_attention(\n","            query=c_emb_unsqueezed,\n","            key=i_emb_unsqueezed,\n","            value=i_emb_unsqueezed\n","        )\n","\n","        # D. Process attention output\n","        attn_output_squeezed = attn_output.squeeze(0) # Shape: (Batch, d_model)\n","        attn_output_dropped = self.dropout_attn(attn_output_squeezed)\n","\n","        # E. Create enriched clinical representation with residual connection and LayerNorm\n","        # c_enriched_by_i = LayerNorm(original_c_emb + attn_output_from_image)\n","        c_enriched_by_i = self.norm_clin(c_emb + attn_output_dropped)\n","\n","        # F. Create fused representation by averaging enriched clinical and original image embeddings\n","        fused = (c_enriched_by_i + i_emb) / 2 # Simple average, or torch.mean(torch.stack([c_enriched_by_i, i_emb]), dim=0)\n","\n","        # G. Predict with fused representation\n","        return self.surv_head(fused).squeeze(-1), self.subtype_head(fused)\n","\n","# --- Best hyperparameters identified from previous steps (using Architectural Config 2 from architectural tuning) ---\n","# The hyperparameters from the architectural tuning step are applied to the _new_ FusionTransformer model.\n","# Best training hyperparameters:\n","best_lr = 0.001\n","best_batch_size = 16\n","best_epochs = 20\n","best_alpha = 0.5\n","# Best architectural hyperparameters (from architectural config 2):\n","best_d_model = 64\n","best_num_heads = 2\n","best_dropout = 0.2\n","\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","\n","print(f\"\\n--- Evaluating Modified FusionTransformer with Cross-Attention ---\")\n","print(f\"Best Hyperparameters: LR={best_lr}, Batch Size={best_batch_size}, Epochs={best_epochs}, Alpha={best_alpha}\")\n","print(f\"Best Architecture: d_model={best_d_model}, num_heads={best_num_heads}, dropout={best_dropout}\")\n","\n","kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","cindex_scores = []\n","roc_auc_scores = []\n","accuracy_scores = []\n","\n","X_kf = master_df[\"patient_id\"].values\n","y_kf = master_df[\"label\"].values # Stratify by subtype label\n","\n","for fold, (tr, va) in enumerate(kf.split(X_kf, y_kf)):\n","    print(f\"\\nFold {fold+1}\")\n","\n","    train_df = master_df.iloc[tr]\n","    val_df   = master_df.iloc[va]\n","\n","    train_loader = make_loader(train_df, batch_size=best_batch_size)\n","    val_loader   = make_loader(val_df, shuffle=False, batch_size=best_batch_size)\n","\n","    model = FusionTransformer(\n","        clin_dim=len(clinical_cols),\n","        img_dim=2048,\n","        d_model=best_d_model,\n","        num_heads=best_num_heads,\n","        dropout=best_dropout\n","    ).to(device)\n","\n","    opt = torch.optim.Adam(model.parameters(), lr=best_lr)\n","\n","    for epoch in range(best_epochs):\n","        model.train()\n","        for b in train_loader:\n","            risk, logits = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","\n","            loss = (\n","                cox_ph_loss(risk, b[\"time\"].to(device), b[\"event\"].to(device))\n","                + best_alpha * F.cross_entropy(logits, b[\"label\"].to(device))\n","            )\n","\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","\n","    # --- Evaluation for current fold ---\n","    model.eval()\n","    all_risk, all_time, all_event = [], [], []\n","    all_logits, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for b in val_loader:\n","            r, l = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","            all_risk.extend(r.cpu().numpy())\n","            all_time.extend(b[\"time\"].numpy())\n","            all_event.extend(b[\"event\"].numpy())\n","            all_logits.append(l.cpu())\n","            all_labels.append(b[\"label\"])\n","\n","    # C-index\n","    cidx = max(\n","        concordance_index(all_time, all_risk, all_event),\n","        concordance_index(all_time, -np.array(all_risk), all_event)\n","    )\n","    cindex_scores.append(cidx)\n","\n","    # Subtype metrics\n","    all_logits = torch.cat(all_logits)\n","    all_labels = torch.cat(all_labels)\n","    probs = F.softmax(all_logits, dim=1).numpy()\n","    y_true = all_labels.numpy()\n","    y_pred = probs.argmax(axis=1)\n","\n","    # Check for single class in validation set for ROC-AUC\n","    if np.unique(y_true).shape[0] < 2:\n","        roc = np.nan # ROC-AUC is undefined for single class\n","    else:\n","        if probs.shape[1] == 2:\n","            roc = roc_auc_score(y_true, probs[:, 1])\n","        else:\n","            roc = roc_auc_score(y_true, probs, multi_class=\"ovr\", average=\"macro\")\n","\n","    acc = accuracy_score(y_true, y_pred)\n","\n","    roc_auc_scores.append(roc)\n","    accuracy_scores.append(acc)\n","\n","    print(f\"  C-index: {cidx:.4f}, ROC-AUC: {roc:.4f}, Accuracy: {acc:.4f}\")\n","\n","mean_cindex = np.mean(cindex_scores)\n","std_cindex = np.std(cindex_scores)\n","mean_roc_auc = np.nanmean(roc_auc_scores) # Use nanmean to handle NaNs if any\n","std_roc_auc = np.nanstd(roc_auc_scores)\n","mean_accuracy = np.mean(accuracy_scores)\n","std_accuracy = np.std(accuracy_scores)\n","\n","print(f\"\\n--- Final Results for Modified FusionTransformer with Cross-Attention ---\")\n","print(f\"  Mean C-index: {mean_cindex:.4f} \\u00B1 {std_cindex:.4f}\")\n","print(f\"  Mean ROC-AUC: {mean_roc_auc:.4f} \\u00B1 {std_roc_auc:.4f}\")\n","print(f\"  Mean Accuracy: {mean_accuracy:.4f} \\u00B1 {std_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229715,"status":"aborted","timestamp":1768302907046,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"66f646c5"},"outputs":[],"source":["class ClinicallyGuidedAttention(nn.Module):\n","    def __init__(self, d_model=128, num_heads=4, dropout=0.1):\n","        super().__init__()\n","\n","        self.cross_attention = nn.MultiheadAttention(\n","            embed_dim=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","            batch_first=False\n","        )\n","\n","        self.norm = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, query_input, key_input, value_input):\n","        # Expand query_input to (1, batch_size, d_model)\n","        query_input_unsqueezed = query_input.unsqueeze(0)\n","\n","        # key_input and value_input are expected to be (sequence_length, batch_size, d_model)\n","        # as required by MultiheadAttention when batch_first=False.\n","        # The caller (FusionTransformer) is responsible for transforming image embeddings into this format.\n","        attn_output, _ = self.cross_attention(\n","            query=query_input_unsqueezed,\n","            key=key_input,\n","            value=value_input\n","        )\n","\n","        # Squeeze the attention output to remove the sequence length dimension (it will be 1)\n","        attn_output_squeezed = attn_output.squeeze(0)\n","\n","        # Apply dropout to the squeezed attention output\n","        attn_output_dropped = self.dropout(attn_output_squeezed)\n","\n","        # Apply layer normalization with a residual connection\n","        final_output = self.norm(query_input + attn_output_dropped)\n","\n","        return final_output"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229726,"status":"aborted","timestamp":1768302907060,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"5a3d0712"},"outputs":[],"source":["class FusionTransformer(nn.Module):\n","    def __init__(self, clin_dim, img_dim=2048, d_model=128, num_heads=4, dropout=0.1, num_img_patches=8):\n","        super().__init__()\n","\n","        # 1. Project clinical data to d_model\n","        self.clin_proj = nn.Linear(clin_dim, d_model)\n","\n","        # 2. Project image embeddings into 'patches' for attention\n","        # We need to reshape the 2048 dim vector into (num_img_patches, d_model)\n","        # So, the linear layer should project from img_dim to (num_img_patches * d_model)\n","        self.img_patch_proj = nn.Linear(img_dim, num_img_patches * d_model)\n","        self.num_img_patches = num_img_patches\n","\n","        # 3. Clinically Guided Attention module\n","        self.guided_attention = ClinicallyGuidedAttention(\n","            d_model=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout\n","        )\n","\n","        # 4. Heads - these will now operate on the output of the guided attention (enriched clinical features)\n","        self.surv_head = nn.Linear(d_model, 1)\n","        self.subtype_head = nn.Linear(d_model, 2) # Assuming 2 classes\n","\n","    def forward(self, clin, img):\n","        # A. Embed clinical features\n","        c_emb = self.clin_proj(clin)  # Shape: (Batch, d_model)\n","\n","        # B. Project image features into patches and reshape\n","        # img_patches_flat: (Batch, num_img_patches * d_model)\n","        img_patches_flat = self.img_patch_proj(img)\n","\n","        # Reshape to (Batch, num_img_patches, d_model)\n","        img_patches = img_patches_flat.view(-1, self.num_img_patches, c_emb.shape[-1])\n","\n","        # C. Prepare for MultiheadAttention (sequence_length, batch_size, embed_dim)\n","        # For ClinicallyGuidedAttention, key_input and value_input are img_patches\n","        # img_patches_seq: (num_img_patches, Batch, d_model)\n","        img_patches_seq = img_patches.permute(1, 0, 2)\n","\n","        # D. Perform Clinically Guided Attention\n","        # query_input = c_emb (Batch, d_model)\n","        # key_input = img_patches_seq (num_img_patches, Batch, d_model)\n","        # value_input = img_patches_seq (num_img_patches, Batch, d_model)\n","        enriched_clin = self.guided_attention(\n","            query_input=c_emb,\n","            key_input=img_patches_seq,\n","            value_input=img_patches_seq\n","        )\n","\n","        # E. Predict with the enriched clinical representation\n","        # The enriched_clin now contains information from the image embeddings, guided by clinical data.\n","        # This single vector (per batch item) acts as the fused representation for prediction.\n","        return self.surv_head(enriched_clin).squeeze(-1), self.subtype_head(enriched_clin)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229713,"status":"aborted","timestamp":1768302907069,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"ea9f7723"},"outputs":[],"source":["class FusionTransformer(nn.Module):\n","    def __init__(self, clin_dim, img_dim=2048, d_model=128, num_heads=4, dropout=0.1, num_img_patches=8):\n","        super().__init__()\n","\n","        # 1. Project clinical data to d_model\n","        self.clin_proj = nn.Linear(clin_dim, d_model)\n","\n","        # 2. Project image embeddings into 'patches' for attention\n","        # We need to reshape the 2048 dim vector into (num_img_patches, d_model)\n","        # So, the linear layer should project from img_dim to (num_img_patches * d_model)\n","        self.img_patch_proj = nn.Linear(img_dim, num_img_patches * d_model)\n","        self.num_img_patches = num_img_patches\n","\n","        # 3. Clinically Guided Attention module\n","        self.guided_attention = ClinicallyGuidedAttention(\n","            d_model=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout\n","        )\n","\n","        # 4. Heads - these will now operate on the output of the guided attention (enriched clinical features)\n","        self.surv_head = nn.Linear(d_model, 1)\n","        self.subtype_head = nn.Linear(d_model, 2) # Assuming 2 classes\n","\n","    def forward(self, clin, img):\n","        # A. Embed clinical features\n","        c_emb = self.clin_proj(clin)  # Shape: (Batch, d_model)\n","\n","        # B. Project image features into patches and reshape\n","        # img_patches_flat: (Batch, num_img_patches * d_model)\n","        img_patches_flat = self.img_patch_proj(img)\n","\n","        # Reshape to (Batch, num_img_patches, d_model)\n","        img_patches = img_patches_flat.view(-1, self.num_img_patches, c_emb.shape[-1])\n","\n","        # C. Prepare for MultiheadAttention (sequence_length, batch_size, embed_dim)\n","        # For ClinicallyGuidedAttention, key_input and value_input are img_patches\n","        # img_patches_seq: (num_img_patches, Batch, d_model)\n","        img_patches_seq = img_patches.permute(1, 0, 2)\n","\n","        # D. Perform Clinically Guided Attention\n","        # query_input = c_emb (Batch, d_model)\n","        # key_input = img_patches_seq (num_img_patches, Batch, d_model)\n","        # value_input = img_patches_seq (num_img_patches, Batch, d_model)\n","        enriched_clin = self.guided_attention(\n","            query_input=c_emb,\n","            key_input=img_patches_seq,\n","            value_input=img_patches_seq\n","        )\n","\n","        # E. Predict with the enriched clinical representation\n","        # The enriched_clin now contains information from the image embeddings, guided by clinical data.\n","        # This single vector (per batch item) acts as the fused representation for prediction.\n","        return self.surv_head(enriched_clin).squeeze(-1), self.subtype_head(enriched_clin)\n","\n","# --- Best hyperparameters identified from previous steps ---\n","# Best training hyperparameters:\n","best_lr = 0.001\n","best_batch_size = 16\n","best_epochs = 20\n","best_alpha = 0.5\n","# Best architectural hyperparameters (from architectural config 2 in previous tuning):\n","best_d_model = 64\n","best_num_heads = 2\n","best_dropout = 0.2\n","\n","# NOTE: num_img_patches is a new hyperparameter for the updated FusionTransformer.\n","# It determines how the 2048-dim image embedding is broken down into 'patches' for attention.\n","# Let's try a reasonable value, e.g., 8, such that 2048 is divisible by d_model * num_img_patches\n","# If d_model=64, num_img_patches * d_model = 8 * 64 = 512. The linear projection is from 2048 to 512. This is incorrect.\n","# The linear layer should project from img_dim to (num_img_patches * d_model).\n","# So img_patch_proj expects img_dim input and outputs num_img_patches * d_model. The current implementation is correct.\n","# Let's set num_img_patches to something that divides into img_dim if we want to literally split the vector,\n","# or just let the linear layer handle the projection. For now, 8 is a good starting point.\n","# The `img_patch_proj` projects `img_dim` (2048) to `num_img_patches * d_model` (8 * 64 = 512).\n","# This means the linear layer directly transforms the 2048-dim image vector into 8 patches of 64 dimensions each.\n","num_img_patches = 8 # This value was already used in the class definition. Reconfirming it here for clarity.\n","\n","print(f\"\\n--- Evaluating Modified FusionTransformer with ClinicallyGuidedAttention ---\")\n","print(f\"Training Hyperparameters: LR={best_lr}, Batch Size={best_batch_size}, Epochs={best_epochs}, Alpha={best_alpha}\")\n","print(f\"Architectural Parameters: d_model={best_d_model}, num_heads={best_num_heads}, dropout={best_dropout}, num_img_patches={num_img_patches}\")\n","\n","kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","cindex_scores = []\n","roc_auc_scores = []\n","accuracy_scores = []\n","\n","X_kf = master_df[\"patient_id\"].values\n","y_kf = master_df[\"label\"].values # Stratify by subtype label\n","\n","for fold, (tr, va) in enumerate(kf.split(X_kf, y_kf)):\n","    print(f\"\\nFold {fold+1}\")\n","\n","    train_df = master_df.iloc[tr]\n","    val_df   = master_df.iloc[va]\n","\n","    train_loader = make_loader(train_df, batch_size=best_batch_size)\n","    val_loader   = make_loader(val_df, shuffle=False, batch_size=best_batch_size)\n","\n","    model = FusionTransformer(\n","        clin_dim=len(clinical_cols),\n","        img_dim=2048,\n","        d_model=best_d_model,\n","        num_heads=best_num_heads,\n","        dropout=best_dropout,\n","        num_img_patches=num_img_patches\n","    ).to(device)\n","\n","    opt = torch.optim.Adam(model.parameters(), lr=best_lr)\n","\n","    for epoch in range(best_epochs):\n","        model.train()\n","        for b in train_loader:\n","            risk, logits = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","\n","            loss = (\n","                cox_ph_loss(risk, b[\"time\"].to(device), b[\"event\"].to(device))\n","                + best_alpha * F.cross_entropy(logits, b[\"label\"].to(device))\n","            )\n","\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","\n","    # --- Evaluation for current fold ---\n","    model.eval()\n","    all_risk, all_time, all_event = [], [], []\n","    all_logits, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for b in val_loader:\n","            r, l = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","            all_risk.extend(r.cpu().numpy())\n","            all_time.extend(b[\"time\"].numpy())\n","            all_event.extend(b[\"event\"].numpy())\n","            all_logits.append(l.cpu())\n","            all_labels.append(b[\"label\"])\n","\n","    # C-index\n","    cidx = max(\n","        concordance_index(all_time, all_risk, all_event),\n","        concordance_index(all_time, -np.array(all_risk), all_event)\n","    )\n","    cindex_scores.append(cidx)\n","\n","    # Subtype metrics\n","    all_logits = torch.cat(all_logits)\n","    all_labels = torch.cat(all_labels)\n","    probs = F.softmax(all_logits, dim=1).numpy()\n","    y_true = all_labels.numpy()\n","    y_pred = probs.argmax(axis=1)\n","\n","    # Check for single class in validation set for ROC-AUC\n","    if np.unique(y_true).shape[0] < 2:\n","        roc = np.nan # ROC-AUC is undefined for single class\n","    else:\n","        if probs.shape[1] == 2:\n","            roc = roc_auc_score(y_true, probs[:, 1])\n","        else:\n","            roc = roc_auc_score(y_true, probs, multi_class=\"ovr\", average=\"macro\")\n","\n","    acc = accuracy_score(y_true, y_pred)\n","\n","    roc_auc_scores.append(roc)\n","    accuracy_scores.append(acc)\n","\n","    print(f\"  C-index: {cidx:.4f}, ROC-AUC: {roc:.4f}, Accuracy: {acc:.4f}\")\n","\n","mean_cindex = np.mean(cindex_scores)\n","std_cindex = np.std(cindex_scores)\n","mean_roc_auc = np.nanmean(roc_auc_scores) # Use nanmean to handle NaNs if any\n","std_roc_auc = np.nanstd(roc_auc_scores)\n","mean_accuracy = np.mean(accuracy_scores)\n","std_accuracy = np.std(accuracy_scores)\n","\n","print(f\"\\n--- Final Results for Modified FusionTransformer with ClinicallyGuidedAttention ---\")\n","print(f\"  Mean C-index: {mean_cindex:.4f} \\u00B1 {std_cindex:.4f}\")\n","print(f\"  Mean ROC-AUC: {mean_roc_auc:.4f} \\u00B1 {std_roc_auc:.4f}\")\n","print(f\"  Mean Accuracy: {mean_accuracy:.4f} \\u00B1 {std_accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"7c1b27a3"},"source":["\n","| Model Variant                                      | C-index (Mean ¬± Std)  | ROC-AUC (Mean ¬± Std) | Accuracy (Mean ¬± Std) | Key Architectural Changes                                                              |\n","| :------------------------------------------------- | :-------------------- | :------------------- | :-------------------- | :------------------------------------------------------------------------------------- |\n","| **1. Initial Baseline (Original FusionTransformer)** | 0.5994 ¬± 0.0651       | 0.4722 (single run)  | 0.6154 (single run)   | Original self-attention between clinical and image tokens.                             |\n","| **2. Tuned Baseline (Best Self-Attention)**        | 0.5840 ¬± 0.0809       | 0.5489 ¬± 0.0780      | 0.6898 ¬± 0.0049       | `d_model=64`, `num_heads=2`, `dropout=0.2`. Still self-attention.                      |\n","| **3. Simpler Cross-Attention**                     | **0.5877 ¬± 0.0609**   | 0.5064 ¬± 0.1255      | 0.6898 ¬± 0.0049       | Clinical as query, image as key/value, then fusion.                                    |\n","| **4. Clinically Guided Attention**                 | 0.5524 ¬± 0.0424       | **0.6004 ¬± 0.1213**  | **0.6898 ¬± 0.0049**   | Clinical as query, image divided into 8 patches as key/value.                          |\n","\n","All tuned models (2, 3, 4) used the same optimized training hyperparameters: Learning Rate (LR) = 0.001, Batch Size = 16, Epochs = 20, Alpha = 0.5.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229699,"status":"aborted","timestamp":1768302907075,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"1e579587"},"outputs":[],"source":["def contrastive_loss(clinical_embeddings, image_embeddings, temperature=0.1):\n","    # 1. L2-normalize embeddings\n","    clinical_embeddings = F.normalize(clinical_embeddings, dim=-1)\n","    image_embeddings = F.normalize(image_embeddings, dim=-1)\n","\n","    # 2. Calculate cosine similarity matrix\n","    # Shape: (batch_size, batch_size)\n","    similarity_matrix = torch.matmul(clinical_embeddings, image_embeddings.T)\n","\n","    # 3. Apply temperature\n","    similarity_matrix = similarity_matrix / temperature\n","\n","    # 4. Compute InfoNCE loss symmetrically\n","\n","    # Clinical-to-Image loss\n","    # Positive samples are on the diagonal (i.e., (clin_i, img_i))\n","    labels = torch.arange(len(similarity_matrix)).to(similarity_matrix.device)\n","    loss_clin_to_img = F.cross_entropy(similarity_matrix, labels)\n","\n","    # Image-to-Clinical loss (transpose similarity_matrix and use the same labels)\n","    loss_img_to_clin = F.cross_entropy(similarity_matrix.T, labels)\n","\n","    # Average the two loss components\n","    total_loss = (loss_clin_to_img + loss_img_to_clin) / 2\n","\n","    return total_loss\n","\n","print(\"Contrastive loss function defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229688,"status":"aborted","timestamp":1768302907082,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"3c86d669"},"outputs":[],"source":["class FusionTransformer(nn.Module):\n","    def __init__(self, clin_dim, img_dim=2048, d_model=128, num_heads=4, dropout=0.1, num_img_patches=8):\n","        super().__init__()\n","\n","        # 1. Project clinical data to d_model\n","        self.clin_proj = nn.Linear(clin_dim, d_model)\n","\n","        # 2. Project image embeddings into 'patches' for attention\n","        self.img_patch_proj = nn.Linear(img_dim, num_img_patches * d_model)\n","        self.num_img_patches = num_img_patches\n","\n","        # New: Projection for raw image embedding to d_model for contrastive loss\n","        self.img_contrast_proj = nn.Linear(img_dim, d_model)\n","\n","        # 3. Clinically Guided Attention module\n","        self.guided_attention = ClinicallyGuidedAttention(\n","            d_model=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout\n","        )\n","\n","        # 4. Heads - these will now operate on the output of the guided attention (enriched clinical features)\n","        self.surv_head = nn.Linear(d_model, 1)\n","        self.subtype_head = nn.Linear(d_model, 2) # Assuming 2 classes\n","\n","    def forward(self, clin, img):\n","        # A. Embed clinical features\n","        c_emb = self.clin_proj(clin)  # Shape: (Batch, d_model)\n","\n","        # B. Project image features into patches and reshape\n","        img_patches_flat = self.img_patch_proj(img)\n","        img_patches = img_patches_flat.view(-1, self.num_img_patches, c_emb.shape[-1])\n","\n","        # C. Prepare for MultiheadAttention\n","        img_patches_seq = img_patches.permute(1, 0, 2)\n","\n","        # D. Perform Clinically Guided Attention\n","        enriched_clin = self.guided_attention(\n","            query_input=c_emb,\n","            key_input=img_patches_seq,\n","            value_input=img_patches_seq\n","        )\n","\n","        # E. Predict with the enriched clinical representation\n","        risk_pred = self.surv_head(enriched_clin).squeeze(-1)\n","        subtype_logits = self.subtype_head(enriched_clin)\n","\n","        # F. Get image embedding projected to d_model for contrastive loss\n","        i_contrast_emb = self.img_contrast_proj(img)\n","\n","        return risk_pred, subtype_logits, c_emb, i_contrast_emb # Return projected embeddings for contrastive loss\n","\n","\n","# --- Best hyperparameters identified from previous steps ---\n","# These hyperparameters were from the previous architectural tuning of the ClinicallyGuidedAttention model.\n","best_lr = 0.001\n","best_batch_size = 16\n","best_epochs = 20\n","best_alpha = 0.5 # Weight for subtype loss\n","\n","best_d_model = 64\n","best_num_heads = 2\n","best_dropout = 0.2\n","num_img_patches = 8 # From ClinicallyGuidedAttention setup\n","\n","# New hyperparameter for contrastive loss weighting\n","beta = 0.1 # Weight for contrastive loss\n","\n","print(f\"\\n--- Evaluating FusionTransformer with ClinicallyGuidedAttention and Contrastive Loss ---\")\n","print(f\"Training Hyperparameters: LR={best_lr}, Batch Size={best_batch_size}, Epochs={best_epochs}, Alpha={best_alpha}, Beta={beta}\")\n","print(f\"Architectural Parameters: d_model={best_d_model}, num_heads={best_num_heads}, dropout={best_dropout}, num_img_patches={num_img_patches}\")\n","\n","kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","cindex_scores = []\n","roc_auc_scores = []\n","accuracy_scores = []\n","\n","X_kf = master_df[\"patient_id\"].values\n","y_kf = master_df[\"label\"].values # Stratify by subtype label\n","\n","for fold, (tr, va) in enumerate(kf.split(X_kf, y_kf)):\n","    print(f\"\\nFold {fold+1}\")\n","\n","    train_df = master_df.iloc[tr]\n","    val_df   = master_df.iloc[va]\n","\n","    train_loader = make_loader(train_df, batch_size=best_batch_size)\n","    val_loader   = make_loader(val_df, shuffle=False, batch_size=best_batch_size)\n","\n","    model = FusionTransformer(\n","        clin_dim=len(clinical_cols),\n","        img_dim=2048,\n","        d_model=best_d_model,\n","        num_heads=best_num_heads,\n","        dropout=best_dropout,\n","        num_img_patches=num_img_patches\n","    ).to(device)\n","\n","    opt = torch.optim.Adam(model.parameters(), lr=best_lr)\n","\n","    for epoch in range(best_epochs):\n","        model.train()\n","        for b in train_loader:\n","            risk, logits, c_proj_emb, i_proj_emb = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","\n","            loss_surv = cox_ph_loss(risk, b[\"time\"].to(device), b[\"event\"].to(device))\n","            loss_sub  = F.cross_entropy(logits, b[\"label\"].to(device))\n","            loss_contrast = contrastive_loss(c_proj_emb, i_proj_emb)\n","\n","            loss = loss_surv + best_alpha * loss_sub + beta * loss_contrast\n","\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","\n","    # --- Evaluation for current fold ---\n","    model.eval()\n","    all_risk, all_time, all_event = [], [], []\n","    all_logits, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for b in val_loader:\n","            r, l, _, _ = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","            all_risk.extend(r.cpu().numpy())\n","            all_time.extend(b[\"time\"].numpy())\n","            all_event.extend(b[\"event\"].numpy())\n","            all_logits.append(l.cpu())\n","            all_labels.append(b[\"label\"])\n","\n","    # C-index\n","    cidx = max(\n","        concordance_index(all_time, all_risk, all_event),\n","        concordance_index(all_time, -np.array(all_risk), all_event)\n","    )\n","    cindex_scores.append(cidx)\n","\n","    # Subtype metrics\n","    all_logits = torch.cat(all_logits)\n","    all_labels = torch.cat(all_labels)\n","    probs = F.softmax(all_logits, dim=1).numpy()\n","    y_true = all_labels.numpy()\n","    y_pred = probs.argmax(axis=1)\n","\n","    # Check for single class in validation set for ROC-AUC\n","    if np.unique(y_true).shape[0] < 2:\n","        roc = np.nan # ROC-AUC is undefined for single class\n","    else:\n","        if probs.shape[1] == 2:\n","            roc = roc_auc_score(y_true, probs[:, 1])\n","        else:\n","            roc = roc_auc_score(y_true, probs, multi_class=\"ovr\", average=\"macro\")\n","\n","    acc = accuracy_score(y_true, y_pred)\n","\n","    roc_auc_scores.append(roc)\n","    accuracy_scores.append(acc)\n","\n","    print(f\"  C-index: {cidx:.4f}, ROC-AUC: {roc:.4f}, Accuracy: {acc:.4f}\")\n","\n","mean_cindex = np.mean(cindex_scores)\n","std_cindex = np.std(cindex_scores)\n","mean_roc_auc = np.nanmean(roc_auc_scores) # Use nanmean to handle NaNs if any\n","std_roc_auc = np.nanstd(roc_auc_scores)\n","mean_accuracy = np.mean(accuracy_scores)\n","std_accuracy = np.std(accuracy_scores)\n","\n","print(f\"\\n--- Final Results for Modified FusionTransformer with ClinicallyGuidedAttention and Contrastive Loss ---\")\n","print(f\"  Mean C-index: {mean_cindex:.4f} \\u00B1 {std_cindex:.4f}\")\n","print(f\"  Mean ROC-AUC: {mean_roc_auc:.4f} \\u00B1 {std_roc_auc:.4f}\")\n","print(f\"  Mean Accuracy: {mean_accuracy:.4f} \\u00B1 {std_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229733,"status":"aborted","timestamp":1768302907130,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"41af8b27"},"outputs":[],"source":["class ClinicallyGuidedAttention(nn.Module):\n","    def __init__(self, d_model=128, num_heads=4, dropout=0.1):\n","        super().__init__()\n","\n","        self.cross_attention = nn.MultiheadAttention(\n","            embed_dim=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","            batch_first=False\n","        )\n","\n","        self.norm = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, query_input, key_input, value_input):\n","        # Expand query_input to (1, batch_size, d_model)\n","        query_input_unsqueezed = query_input.unsqueeze(0)\n","\n","        # key_input and value_input are expected to be (sequence_length, batch_size, d_model)\n","        # as required by MultiheadAttention when batch_first=False.\n","        # The caller (FusionTransformer) is responsible for transforming image embeddings into this format.\n","        attn_output, attn_weights = self.cross_attention(\n","            query=query_input_unsqueezed,\n","            key=key_input,\n","            value=value_input\n","        )\n","\n","        # Squeeze the attention output to remove the sequence length dimension (it will be 1)\n","        attn_output_squeezed = attn_output.squeeze(0)\n","\n","        # Apply dropout to the squeezed attention output\n","        attn_output_dropped = self.dropout(attn_output_squeezed)\n","\n","        # Apply layer normalization with a residual connection\n","        final_output = self.norm(query_input + attn_output_dropped)\n","\n","        return final_output, attn_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229714,"status":"aborted","timestamp":1768302907135,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"386a2f74"},"outputs":[],"source":["class FusionTransformer(nn.Module):\n","    def __init__(self, clin_dim, img_dim=2048, d_model=128, num_heads=4, dropout=0.1, num_img_patches=8):\n","        super().__init__()\n","\n","        # 1. Project clinical data to d_model\n","        self.clin_proj = nn.Linear(clin_dim, d_model)\n","\n","        # 2. Project image embeddings into 'patches' for attention\n","        self.img_patch_proj = nn.Linear(img_dim, num_img_patches * d_model)\n","        self.num_img_patches = num_img_patches\n","\n","        # New: Projection for raw image embedding to d_model for contrastive loss\n","        self.img_contrast_proj = nn.Linear(img_dim, d_model)\n","\n","        # 3. Clinically Guided Attention module\n","        self.guided_attention = ClinicallyGuidedAttention(\n","            d_model=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout\n","        )\n","\n","        # 4. Heads - these will now operate on the output of the guided attention (enriched clinical features)\n","        self.surv_head = nn.Linear(d_model, 1)\n","        self.subtype_head = nn.Linear(d_model, 2) # Assuming 2 classes\n","\n","    def forward(self, clin, img):\n","        # A. Embed clinical features\n","        c_emb = self.clin_proj(clin)  # Shape: (Batch, d_model)\n","\n","        # B. Project image features into patches and reshape\n","        img_patches_flat = self.img_patch_proj(img)\n","        img_patches = img_patches_flat.view(-1, self.num_img_patches, c_emb.shape[-1])\n","\n","        # C. Prepare for MultiheadAttention\n","        img_patches_seq = img_patches.permute(1, 0, 2)\n","\n","        # D. Perform Clinically Guided Attention\n","        enriched_clin, attn_weights = self.guided_attention(\n","            query_input=c_emb,\n","            key_input=img_patches_seq,\n","            value_input=img_patches_seq\n","        )\n","\n","        # E. Predict with the enriched clinical representation\n","        risk_pred = self.surv_head(enriched_clin).squeeze(-1)\n","        subtype_logits = self.subtype_head(enriched_clin)\n","\n","        # F. Get image embedding projected to d_model for contrastive loss\n","        i_contrast_emb = self.img_contrast_proj(img)\n","\n","        return risk_pred, subtype_logits, c_emb, i_contrast_emb, attn_weights # Return projected embeddings for contrastive loss and attention weights\n","\n","\n","# --- Best hyperparameters identified from previous steps ---\n","# These hyperparameters were from the previous architectural tuning of the ClinicallyGuidedAttention model.\n","best_lr = 0.001\n","best_batch_size = 16\n","best_epochs = 20\n","best_alpha = 0.5 # Weight for subtype loss\n","\n","best_d_model = 64\n","best_num_heads = 2\n","best_dropout = 0.2\n","num_img_patches = 8 # From ClinicallyGuidedAttention setup\n","\n","# New hyperparameter for contrastive loss weighting\n","beta = 0.1 # Weight for contrastive loss\n","\n","print(f\"\\n--- Evaluating FusionTransformer with ClinicallyGuidedAttention and Contrastive Loss ---\")\n","print(f\"Training Hyperparameters: LR={best_lr}, Batch Size={best_batch_size}, Epochs={best_epochs}, Alpha={best_alpha}, Beta={beta}\")\n","print(f\"Architectural Parameters: d_model={best_d_model}, num_heads={best_num_heads}, dropout={best_dropout}, num_img_patches={num_img_patches}\")\n","\n","kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","cindex_scores = []\n","roc_auc_scores = []\n","accuracy_scores = []\n","\n","X_kf = master_df[\"patient_id\"].values\n","y_kf = master_df[\"label\"].values # Stratify by subtype label\n","\n","for fold, (tr, va) in enumerate(kf.split(X_kf, y_kf)):\n","    print(f\"\\nFold {fold+1}\")\n","\n","    train_df = master_df.iloc[tr]\n","    val_df   = master_df.iloc[va]\n","\n","    train_loader = make_loader(train_df, batch_size=best_batch_size)\n","    val_loader   = make_loader(val_df, shuffle=False, batch_size=best_batch_size)\n","\n","    model = FusionTransformer(\n","        clin_dim=len(clinical_cols),\n","        img_dim=2048,\n","        d_model=best_d_model,\n","        num_heads=best_num_heads,\n","        dropout=best_dropout,\n","        num_img_patches=num_img_patches\n","    ).to(device)\n","\n","    opt = torch.optim.Adam(model.parameters(), lr=best_lr)\n","\n","    for epoch in range(best_epochs):\n","        model.train()\n","        for b in train_loader:\n","            risk, logits, c_proj_emb, i_proj_emb, _ = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","\n","            loss_surv = cox_ph_loss(risk, b[\"time\"].to(device), b[\"event\"].to(device))\n","            loss_sub  = F.cross_entropy(logits, b[\"label\"].to(device))\n","            loss_contrast = contrastive_loss(c_proj_emb, i_proj_emb)\n","\n","            loss = loss_surv + best_alpha * loss_sub + beta * loss_contrast\n","\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","\n","    # --- Evaluation for current fold ---\n","    model.eval()\n","    all_risk, all_time, all_event = [], [], []\n","    all_logits, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for b in val_loader:\n","            r, l, _, _, _ = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","            all_risk.extend(r.cpu().numpy())\n","            all_time.extend(b[\"time\"].numpy())\n","            all_event.extend(b[\"event\"].numpy())\n","            all_logits.append(l.cpu())\n","            all_labels.append(b[\"label\"])\n","\n","    # C-index\n","    cidx = max(\n","        concordance_index(all_time, all_risk, all_event),\n","        concordance_index(all_time, -np.array(all_risk), all_event)\n","    )\n","    cindex_scores.append(cidx)\n","\n","    # Subtype metrics\n","    all_logits = torch.cat(all_logits)\n","    all_labels = torch.cat(all_labels)\n","    probs = F.softmax(all_logits, dim=1).numpy()\n","    y_true = all_labels.numpy()\n","    y_pred = probs.argmax(axis=1)\n","\n","    # Check for single class in validation set for ROC-AUC\n","    if np.unique(y_true).shape[0] < 2:\n","        roc = np.nan # ROC-AUC is undefined for single class\n","    else:\n","        if probs.shape[1] == 2:\n","            roc = roc_auc_score(y_true, probs[:, 1])\n","        else:\n","            roc = roc_auc_score(y_true, probs, multi_class=\"ovr\", average=\"macro\")\n","\n","    acc = accuracy_score(y_true, y_pred)\n","\n","    roc_auc_scores.append(roc)\n","    accuracy_scores.append(acc)\n","\n","    print(f\"  C-index: {cidx:.4f}, ROC-AUC: {roc:.4f}, Accuracy: {acc:.4f}\")\n","\n","mean_cindex = np.mean(cindex_scores)\n","std_cindex = np.std(cindex_scores)\n","mean_roc_auc = np.nanmean(roc_auc_scores) # Use nanmean to handle NaNs if any\n","std_roc_auc = np.nanstd(roc_auc_scores)\n","mean_accuracy = np.mean(accuracy_scores)\n","std_accuracy = np.std(accuracy_scores)\n","\n","print(f\"\\n--- Final Results for Modified FusionTransformer with ClinicallyGuidedAttention and Contrastive Loss ---\")\n","print(f\"  Mean C-index: {mean_cindex:.4f} \\u00B1 {std_cindex:.4f}\")\n","print(f\"  Mean ROC-AUC: {mean_roc_auc:.4f} \\u00B1 {std_roc_auc:.4f}\")\n","print(f\"  Mean Accuracy: {mean_accuracy:.4f} \\u00B1 {std_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229701,"status":"aborted","timestamp":1768302907141,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"ff3b652c"},"outputs":[],"source":["tnbc_patient = master_df[master_df['label'] == 0].iloc[0]\n","her2_pos_patient = master_df[master_df['label'] == 1].iloc[0]\n","\n","selected_patients_df = pd.DataFrame([tnbc_patient, her2_pos_patient]).reset_index(drop=True)\n","\n","print(\"Selected Patients for Visualization:\")\n","print(selected_patients_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229688,"status":"aborted","timestamp":1768302907145,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"2bd6c00f"},"outputs":[],"source":["vis_loader = make_loader(selected_patients_df, shuffle=False, batch_size=2)\n","\n","# Load the model with the best architectural parameters\n","vis_model = FusionTransformer(\n","    clin_dim=len(clinical_cols),\n","    img_dim=2048,\n","    d_model=best_d_model,\n","    num_heads=best_num_heads,\n","    dropout=best_dropout,\n","    num_img_patches=num_img_patches\n",").to(device)\n","\n","# NOTE: This model has not been trained yet in this notebook execution, only its architecture has been defined.\n","# For attention visualization, it's best to use a *trained* model.\n","# Since training is done within the K-fold loop, we can re-train a model here or assume an existing trained model.\n","# For demonstration, we will re-initialize and perform a dummy forward pass to get weight shapes.\n","\n","# Set model to evaluation mode\n","vis_model.eval()\n","\n","# Perform a forward pass to get attention weights\n","all_attn_weights = []\n","\n","with torch.no_grad():\n","    for b in vis_loader:\n","        clin_data = b[\"clin\"].to(device)\n","        img_data = b[\"img\"].to(device)\n","\n","        # The forward pass now returns attn_weights\n","        risk, logits, c_proj_emb, i_proj_emb, attn_weights = vis_model(\n","            clin_data,\n","            img_data\n","        )\n","        all_attn_weights.append(attn_weights.cpu().numpy())\n","\n","# Concatenate all attention weights if batch_size > 1 and multiple batches\n","if len(all_attn_weights) > 1:\n","    final_attn_weights = np.concatenate(all_attn_weights, axis=0)\n","else:\n","    final_attn_weights = all_attn_weights[0]\n","\n","\n","print(\"Shape of attention weights:\", final_attn_weights.shape)\n","print(\"Example attention weights (first patient, first head, all patches):\\n\", final_attn_weights[0, 0, :])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229689,"status":"aborted","timestamp":1768302907148,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"cd608999"},"outputs":[],"source":["class ClinicallyGuidedAttention(nn.Module):\n","    def __init__(self, d_model=128, num_heads=4, dropout=0.1):\n","        super().__init__()\n","\n","        self.cross_attention = nn.MultiheadAttention(\n","            embed_dim=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","            batch_first=True, # Changed to True\n","            average_attn_weights=False # Changed to False to get per-head weights\n","        )\n","\n","        self.norm = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, query_input, key_input, value_input):\n","        # query_input is (Batch, d_model), so unsqueeze to (Batch, 1, d_model) for batch_first=True MHA\n","        query_input_expanded = query_input.unsqueeze(1)\n","\n","        # key_input and value_input are (num_img_patches, Batch, d_model) from FusionTransformer\n","        # Permute to (Batch, num_img_patches, d_model) for batch_first=True MHA\n","        key_input_rearranged = key_input.permute(1, 0, 2)\n","        value_input_rearranged = value_input.permute(1, 0, 2)\n","\n","        attn_output, attn_weights = self.cross_attention(\n","            query=query_input_expanded,\n","            key=key_input_rearranged,\n","            value=value_input_rearranged\n","        )\n","\n","        # attn_output shape is (Batch, 1, d_model) because query seq_len is 1\n","        attn_output_squeezed = attn_output.squeeze(1)\n","\n","        # Apply dropout\n","        attn_output_dropped = self.dropout(attn_output_squeezed)\n","\n","        # Apply layer normalization with a residual connection\n","        final_output = self.norm(query_input + attn_output_dropped)\n","\n","        # attn_weights shape will be (Batch, num_heads, query_seq_len, key_seq_len)\n","        # which is (Batch, num_heads, 1, num_img_patches). Squeeze the query_seq_len dimension.\n","        return final_output, attn_weights.squeeze(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229693,"status":"aborted","timestamp":1768302907154,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"b1a4e7f8"},"outputs":[],"source":["class ClinicallyGuidedAttention(nn.Module):\n","    def __init__(self, d_model=128, num_heads=4, dropout=0.1):\n","        super().__init__()\n","\n","        self.cross_attention = nn.MultiheadAttention(\n","            embed_dim=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","            batch_first=True # Changed to True\n","            # Removed average_attn_weights=False as it's not supported in this PyTorch version\n","        )\n","\n","        self.norm = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, query_input, key_input, value_input):\n","        # query_input is (Batch, d_model), so unsqueeze to (Batch, 1, d_model) for batch_first=True MHA\n","        query_input_expanded = query_input.unsqueeze(1)\n","\n","        # key_input and value_input are (num_img_patches, Batch, d_model) from FusionTransformer\n","        # Permute to (Batch, num_img_patches, d_model) for batch_first=True MHA\n","        key_input_rearranged = key_input.permute(1, 0, 2)\n","        value_input_rearranged = value_input.permute(1, 0, 2)\n","\n","        attn_output, attn_weights = self.cross_attention(\n","            query=query_input_expanded,\n","            key=key_input_rearranged,\n","            value=value_input_rearranged\n","        )\n","\n","        # attn_output shape is (Batch, 1, d_model) because query seq_len is 1\n","        attn_output_squeezed = attn_output.squeeze(1)\n","\n","        # Apply dropout\n","        attn_output_dropped = self.dropout(attn_output_squeezed)\n","\n","        # Apply layer normalization with a residual connection\n","        final_output = self.norm(query_input + attn_output_dropped)\n","\n","        # attn_weights shape will be (Batch, num_heads, query_seq_len, key_seq_len)\n","        # which is (Batch, num_heads, 1, num_img_patches). Squeeze the query_seq_len dimension.\n","        return final_output, attn_weights.squeeze(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229680,"status":"aborted","timestamp":1768302907159,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"6d3a9e59"},"outputs":[],"source":["class FusionTransformer(nn.Module):\n","    def __init__(self, clin_dim, img_dim=2048, d_model=128, num_heads=4, dropout=0.1, num_img_patches=8):\n","        super().__init__()\n","\n","        # 1. Project clinical data to d_model\n","        self.clin_proj = nn.Linear(clin_dim, d_model)\n","\n","        # 2. Project image embeddings into 'patches' for attention\n","        self.img_patch_proj = nn.Linear(img_dim, num_img_patches * d_model)\n","        self.num_img_patches = num_img_patches\n","\n","        # New: Projection for raw image embedding to d_model for contrastive loss\n","        self.img_contrast_proj = nn.Linear(img_dim, d_model)\n","\n","        # 3. Clinically Guided Attention module\n","        self.guided_attention = ClinicallyGuidedAttention(\n","            d_model=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout\n","        )\n","\n","        # 4. Heads - these will now operate on the output of the guided attention (enriched clinical features)\n","        self.surv_head = nn.Linear(d_model, 1)\n","        self.subtype_head = nn.Linear(d_model, 2) # Assuming 2 classes\n","\n","    def forward(self, clin, img):\n","        # A. Embed clinical features\n","        c_emb = self.clin_proj(clin)  # Shape: (Batch, d_model)\n","\n","        # B. Project image features into patches and reshape\n","        img_patches_flat = self.img_patch_proj(img)\n","        img_patches = img_patches_flat.view(-1, self.num_img_patches, c_emb.shape[-1])\n","\n","        # C. Prepare for MultiheadAttention\n","        img_patches_seq = img_patches.permute(1, 0, 2)\n","\n","        # D. Perform Clinically Guided Attention\n","        enriched_clin, attn_weights = self.guided_attention(\n","            query_input=c_emb,\n","            key_input=img_patches_seq,\n","            value_input=img_patches_seq\n","        )\n","\n","        # E. Predict with the enriched clinical representation\n","        risk_pred = self.surv_head(enriched_clin).squeeze(-1)\n","        subtype_logits = self.subtype_head(enriched_clin)\n","\n","        # F. Get image embedding projected to d_model for contrastive loss\n","        i_contrast_emb = self.img_contrast_proj(img)\n","\n","        return risk_pred, subtype_logits, c_emb, i_contrast_emb, attn_weights # Return projected embeddings for contrastive loss and attention weights\n","\n","\n","# --- Best hyperparameters identified from previous steps ---\n","# Best training hyperparameters:\n","best_lr = 0.001\n","best_batch_size = 16\n","best_epochs = 20\n","best_alpha = 0.5 # Weight for subtype loss\n","\n","best_d_model = 64\n","best_num_heads = 2\n","best_dropout = 0.2\n","num_img_patches = 8 # From ClinicallyGuidedAttention setup\n","\n","# New hyperparameter for contrastive loss weighting\n","beta = 0.1 # Weight for contrastive loss\n","\n","print(f\"\\n--- Evaluating FusionTransformer with ClinicallyGuidedAttention and Contrastive Loss ---\")\n","print(f\"Training Hyperparameters: LR={best_lr}, Batch Size={best_batch_size}, Epochs={best_epochs}, Alpha={best_alpha}, Beta={beta}\")\n","print(f\"Architectural Parameters: d_model={best_d_model}, num_heads={best_num_heads}, dropout={best_dropout}, num_img_patches={num_img_patches}\")\n","\n","kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","cindex_scores = []\n","roc_auc_scores = []\n","accuracy_scores = []\n","\n","X_kf = master_df[\"patient_id\"].values\n","y_kf = master_df[\"label\"].values # Stratify by subtype label\n","\n","for fold, (tr, va) in enumerate(kf.split(X_kf, y_kf)):\n","    print(f\"\\nFold {fold+1}\")\n","\n","    train_df = master_df.iloc[tr]\n","    val_df   = master_df.iloc[va]\n","\n","    train_loader = make_loader(train_df, batch_size=best_batch_size)\n","    val_loader   = make_loader(val_df, shuffle=False, batch_size=best_batch_size)\n","\n","    model = FusionTransformer(\n","        clin_dim=len(clinical_cols),\n","        img_dim=2048,\n","        d_model=best_d_model,\n","        num_heads=best_num_heads,\n","        dropout=best_dropout,\n","        num_img_patches=num_img_patches\n","    ).to(device)\n","\n","    opt = torch.optim.Adam(model.parameters(), lr=best_lr)\n","\n","    for epoch in range(best_epochs):\n","        model.train()\n","        for b in train_loader:\n","            risk, logits, c_proj_emb, i_proj_emb, _ = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","\n","            loss_surv = cox_ph_loss(risk, b[\"time\"].to(device), b[\"event\"].to(device))\n","            loss_sub  = F.cross_entropy(logits, b[\"label\"].to(device))\n","            loss_contrast = contrastive_loss(c_proj_emb, i_proj_emb)\n","\n","            loss = loss_surv + best_alpha * loss_sub + beta * loss_contrast\n","\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","\n","    # --- Evaluation for current fold ---\n","    model.eval()\n","    all_risk, all_time, all_event = [], [], []\n","    all_logits, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for b in val_loader:\n","            r, l, _, _, _ = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","            all_risk.extend(r.cpu().numpy())\n","            all_time.extend(b[\"time\"].numpy())\n","            all_event.extend(b[\"event\"].numpy())\n","            all_logits.append(l.cpu())\n","            all_labels.append(b[\"label\"])\n","\n","    # C-index\n","    cidx = max(\n","        concordance_index(all_time, all_risk, all_event),\n","        concordance_index(all_time, -np.array(all_risk), all_event)\n","    )\n","    cindex_scores.append(cidx)\n","\n","    # Subtype metrics\n","    all_logits = torch.cat(all_logits)\n","    all_labels = torch.cat(all_labels)\n","    probs = F.softmax(all_logits, dim=1).numpy()\n","    y_true = all_labels.numpy()\n","    y_pred = probs.argmax(axis=1)\n","\n","    # Check for single class in validation set for ROC-AUC\n","    if np.unique(y_true).shape[0] < 2:\n","        roc = np.nan # ROC-AUC is undefined for single class\n","    else:\n","        if probs.shape[1] == 2:\n","            roc = roc_auc_score(y_true, probs[:, 1])\n","        else:\n","            roc = roc_auc_score(y_true, probs, multi_class=\"ovr\", average=\"macro\")\n","\n","    acc = accuracy_score(y_true, y_pred)\n","\n","    roc_auc_scores.append(roc)\n","    accuracy_scores.append(acc)\n","\n","    print(f\"  C-index: {cidx:.4f}, ROC-AUC: {roc:.4f}, Accuracy: {acc:.4f}\")\n","\n","mean_cindex = np.mean(cindex_scores)\n","std_cindex = np.std(cindex_scores)\n","mean_roc_auc = np.nanmean(roc_auc_scores) # Use nanmean to handle NaNs if any\n","std_roc_auc = np.nanstd(roc_auc_scores)\n","mean_accuracy = np.mean(accuracy_scores)\n","std_accuracy = np.std(accuracy_scores)\n","\n","print(f\"\\n--- Final Results for Modified FusionTransformer with ClinicallyGuidedAttention and Contrastive Loss ---\")\n","print(f\"  Mean C-index: {mean_cindex:.4f} \\u00B1 {std_cindex:.4f}\")\n","print(f\"  Mean ROC-AUC: {mean_roc_auc:.4f} \\u00B1 {std_roc_auc:.4f}\")\n","print(f\"  Mean Accuracy: {mean_accuracy:.4f} \\u00B1 {std_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229660,"status":"aborted","timestamp":1768302907163,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"acc776cd"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import io\n","import base64\n","from IPython.display import HTML, display\n","\n","def visualize_attention_maps(attention_weights_per_patient, patient_ids, num_patches=8):\n","    actual_num_heads = attention_weights_per_patient.shape[1]\n","\n","    patch_grid_rows = 2\n","    patch_grid_cols = num_patches // patch_grid_rows\n","    num_patients = len(patient_ids)\n","\n","    fig, axes = plt.subplots(\n","        num_patients,\n","        max(1, actual_num_heads), # Ensure at least 1 column\n","        figsize=(5 * actual_num_heads, 4 * num_patients),\n","        squeeze=False\n","    )\n","    fig.suptitle(f'Radiogenomic Attention Maps ({actual_num_heads} Head detected)', fontsize=16)\n","\n","    for i, pid in enumerate(patient_ids):\n","        patient_attn_weights = attention_weights_per_patient[i]\n","\n","        for head_idx in range(actual_num_heads):\n","            head_weights = patient_attn_weights[head_idx].reshape(patch_grid_rows, patch_grid_cols)\n","\n","            ax = axes[i, head_idx]\n","            sns.heatmap(\n","                head_weights,\n","                ax=ax,\n","                cmap='magma',\n","                annot=True,\n","                fmt=\".2f\",\n","                cbar=True\n","            )\n","            ax.set_title(f\"Patient {pid}\\nAttention Head {head_idx+1}\")\n","            ax.axis('off')\n","\n","    plt.tight_layout()\n","\n","    # Save plot to a BytesIO object and display it as an image\n","    buf = io.BytesIO()\n","    plt.savefig(buf, format='png')\n","    buf.seek(0)\n","    img_str = base64.b64encode(buf.read()).decode('utf-8')\n","    plt.close(fig) # Close the plot to free memory\n","\n","    display(HTML(f'<img src=\"data:image/png;base64,{img_str}\"/>'))\n","\n","# Call the function with the extracted attention weights and patient IDs\n","visualize_attention_maps(\n","    attention_weights_per_patient=final_attn_weights,\n","    patient_ids=selected_patients_df['patient_id'].values,\n","    num_patches=num_img_patches # This is 8 from the context\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229628,"status":"aborted","timestamp":1768302907167,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"6ed65de7"},"outputs":[],"source":["class RadiogenomicGatedAttention(nn.Module):\n","    def __init__(self, clin_dim, img_dim, d_model=128, dropout=0.1, num_img_patches=8, num_heads=4):\n","        super().__init__()\n","\n","        # 1. Project clinical data to d_model\n","        self.clin_proj = nn.Linear(clin_dim, d_model)\n","\n","        # 2. Project image embeddings into 'patches' for attention\n","        self.img_patch_proj = nn.Linear(img_dim, num_img_patches * d_model)\n","        self.num_img_patches = num_img_patches\n","\n","        # 3. Gating mechanism for clinical features\n","        self.clin_gate_linear = nn.Linear(d_model, d_model)\n","        self.clin_gate_sigmoid = nn.Sigmoid()\n","\n","        # 4. Clinically Guided Attention module\n","        self.guided_attention = ClinicallyGuidedAttention(\n","            d_model=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout\n","        )\n","\n","        # 5. Heads for survival and subtype prediction\n","        self.surv_head = nn.Linear(d_model, 1)\n","        self.subtype_head = nn.Linear(d_model, 2) # Assuming 2 classes\n","\n","    def forward(self, clin, img):\n","        # A. Embed clinical features\n","        c_emb = self.clin_proj(clin)  # Shape: (Batch, d_model)\n","\n","        # B. Apply gating to clinical features\n","        gate = self.clin_gate_sigmoid(self.clin_gate_linear(c_emb))\n","        gated_c_emb = c_emb * gate  # Apply gate to clinical embedding\n","\n","        # C. Project image features into patches and reshape\n","        img_patches_flat = self.img_patch_proj(img)\n","        img_patches = img_patches_flat.view(-1, self.num_img_patches, gated_c_emb.shape[-1])\n","\n","        # D. Prepare for MultiheadAttention (sequence_length, batch_size, embed_dim)\n","        img_patches_seq = img_patches.permute(1, 0, 2)\n","\n","        # E. Perform Clinically Guided Attention\n","        # query_input is now the gated clinical embedding\n","        enriched_clin, attn_weights = self.guided_attention(\n","            query_input=gated_c_emb,\n","            key_input=img_patches_seq,\n","            value_input=img_patches_seq\n","        )\n","\n","        # F. Predict with the enriched clinical representation\n","        risk_pred = self.surv_head(enriched_clin).squeeze(-1)\n","        subtype_logits = self.subtype_head(enriched_clin)\n","\n","        # For contrastive loss, we will need the projected clinical and image embeddings\n","        # (though not directly returned by this forward pass for now, as it's not requested in the subtask)\n","        # If needed, `c_emb` and a projected `img` could be returned.\n","\n","        return risk_pred, subtype_logits, attn_weights\n","\n","print(\"RadiogenomicGatedAttention class defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":229646,"status":"aborted","timestamp":1768302907204,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"d5592f4b"},"outputs":[],"source":["class SurvivalDataset(Dataset):\n","    def __init__(self, df, clinical_df):\n","        self.df = df.reset_index(drop=True)\n","        self.clin_features = clinical_df[clinical_cols]\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        r = self.df.iloc[idx]\n","        pid = str(r[\"patient_id\"])\n","\n","        img = np.load(r[\"img_path\"]).astype(\"float32\")\n","        clin = self.clin_features.loc[pid].values.astype(\"float32\")\n","\n","        return {\n","            \"img\": torch.tensor(img),\n","            \"clin\": torch.tensor(clin),\n","            \"time\": torch.tensor(r[\"time\"], dtype=torch.float32),\n","            \"event\": torch.tensor(r[\"event\"], dtype=torch.float32),\n","            \"label\": torch.tensor(r[\"label\"], dtype=torch.long) # Changed back to 'label'\n","        }\n","\n","def make_loader(df, shuffle=True, batch_size=8):\n","    ds = SurvivalDataset(df, clinical_df)\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=False\n","    )\n","\n","# --- Best hyperparameters identified from previous steps (from architectural config 2) ---\n","# The hyperparameters from the architectural tuning step are applied to the new RadiogenomicGatedAttention model.\n","# Best training hyperparameters:\n","best_lr = 0.001\n","best_batch_size = 16\n","best_epochs = 20\n","best_alpha = 0.5 # Weight for treatment response loss (formerly subtype loss)\n","# Best architectural hyperparameters:\n","best_d_model = 64\n","best_num_heads = 2\n","best_dropout = 0.2\n","num_img_patches = 8\n","\n","print(f\"\\n--- Evaluating RadiogenomicGatedAttention with Best Hyperparameters ---\")\n","print(f\"Training Hyperparameters: LR={best_lr}, Batch Size={best_batch_size}, Epochs={best_epochs}, Alpha={best_alpha}\")\n","print(f\"Architectural Parameters: d_model={best_d_model}, num_heads={best_num_heads}, dropout={best_dropout}, num_img_patches={num_img_patches}\")\n","\n","kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","cindex_scores = []\n","roc_auc_scores = []\n","accuracy_scores = []\n","\n","X_kf = master_df[\"patient_id\"].values\n","y_kf = master_df[\"label\"].values # Stratify by 'label' (HR/HER2 subtype) for consistent splits\n","\n","for fold, (tr, va) in enumerate(kf.split(X_kf, y_kf)):\n","    print(f\"\\nFold {fold+1}\")\n","\n","    train_df = master_df.iloc[tr]\n","    val_df   = master_df.iloc[va]\n","\n","    train_loader = make_loader(train_df, batch_size=best_batch_size)\n","    val_loader   = make_loader(val_df, shuffle=False, batch_size=best_batch_size)\n","\n","    model = RadiogenomicGatedAttention(\n","        clin_dim=len(clinical_cols),\n","        img_dim=2048,\n","        d_model=best_d_model,\n","        num_heads=best_num_heads,\n","        dropout=best_dropout,\n","        num_img_patches=num_img_patches\n","    ).to(device)\n","\n","    opt = torch.optim.Adam(model.parameters(), lr=best_lr)\n","\n","    for epoch in range(best_epochs):\n","        model.train()\n","        for b in train_loader:\n","            risk_pred, subtype_logits, _ = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","\n","            loss_surv = cox_ph_loss(risk_pred, b[\"time\"].to(device), b[\"event\"].to(device))\n","            loss_subtype  = F.cross_entropy(subtype_logits, b[\"label\"].to(device)) # Using 'label'\n","\n","            loss = loss_surv + best_alpha * loss_subtype\n","\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","\n","    # --- Evaluation for current fold ---\n","    model.eval()\n","    all_risk, all_time, all_event = [], [], []\n","    all_subtype_logits, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for b in val_loader:\n","            r, l, _ = model(\n","                b[\"clin\"].to(device),\n","                b[\"img\"].to(device)\n","            )\n","            all_risk.extend(r.cpu().numpy())\n","            all_time.extend(b[\"time\"].numpy())\n","            all_event.extend(b[\"event\"].numpy())\n","            all_subtype_logits.append(l.cpu())\n","            all_labels.append(b[\"label\"]) # Changed back to 'label'\n","\n","    # C-index\n","    cidx = max(\n","        concordance_index(all_time, all_risk, all_event),\n","        concordance_index(all_time, -np.array(all_risk), all_event)\n","    )\n","    cindex_scores.append(cidx)\n","\n","    # Subtype metrics\n","    all_subtype_logits = torch.cat(all_subtype_logits)\n","    all_labels = torch.cat(all_labels)\n","    probs = F.softmax(all_subtype_logits, dim=1).numpy()\n","    y_true = all_labels.numpy()\n","    y_pred = probs.argmax(axis=1)\n","\n","    # Check for single class in validation set for ROC-AUC\n","    if np.unique(y_true).shape[0] < 2:\n","        roc = np.nan # ROC-AUC is undefined for single class\n","    else:\n","        if probs.shape[1] == 2:\n","            roc = roc_auc_score(y_true, probs[:, 1])\n","        else:\n","            roc = roc_auc_score(y_true, probs, multi_class=\"ovr\", average=\"macro\")\n","\n","    acc = accuracy_score(y_true, y_pred)\n","\n","    roc_auc_scores.append(roc)\n","    accuracy_scores.append(acc)\n","\n","    print(f\"  C-index: {cidx:.4f}, ROC-AUC: {roc:.4f}, Accuracy: {acc:.4f}\")\n","\n","mean_cindex = np.mean(cindex_scores)\n","std_cindex = np.std(cindex_scores)\n","mean_roc_auc = np.nanmean(roc_auc_scores) # Use nanmean to handle NaNs if any\n","std_roc_auc = np.nanstd(roc_auc_scores)\n","mean_accuracy = np.mean(accuracy_scores)\n","std_accuracy = np.std(accuracy_scores)\n","\n","print(f\"\\n--- Final Results for RadiogenomicGatedAttention ---\")\n","print(f\"  Mean C-index: {mean_cindex:.4f} \\u00B1 {std_cindex:.4f}\")\n","print(f\"  Mean ROC-AUC: {mean_roc_auc:.4f} \\u00B1 {std_roc_auc:.4f}\")\n","print(f\"  Mean Accuracy: {mean_accuracy:.4f} \\u00B1 {std_accuracy:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"9c7c64f8"},"source":["\n","### Performance Comparison\n","\n","| Model Variant                                      | C-index (Mean ¬± Std)  | ROC-AUC (Mean ¬± Std) | Accuracy (Mean ¬± Std) | Key Architectural Changes                                                              |\n","| :------------------------------------------------- | :-------------------- | :------------------- | :-------------------- | :------------------------------------------------------------------------------------- |\n","| **1. Initial Baseline (Original FusionTransformer)** | 0.5994 ¬± 0.0651       | 0.4722 (single run)  | 0.6154 (single run)   | Original self-attention between clinical and image tokens.                             |\n","| **2. Tuned Baseline (Best Self-Attention)**        | 0.5840 ¬± 0.0809       | 0.5489 ¬± 0.0780      | 0.6898 ¬± 0.0049       | `d_model=64`, `num_heads=2`, `dropout=0.2`. Still self-attention.                      |\n","| **3. Simpler Cross-Attention**                     | **0.5877 ¬± 0.0609**   | 0.5064 ¬± 0.1255      | 0.6898 ¬± 0.0049       | Clinical as query, image as key/value, then fusion.                                    |\n","| **4. Clinically Guided Attention**                 | 0.5524 ¬± 0.0424       | **0.6004 ¬± 0.1213**  | **0.6898 ¬± 0.0049**   | Clinical as query, image divided into 8 patches as key/value.                          |\n","| **5. Radiogenomic Gated Attention**                | 0.5662 ¬± 0.0419       | 0.4806 ¬± 0.1442      | 0.6898 ¬± 0.0049       | Gating mechanism on clinical features before Clinically Guided Attention.              |\n","\n"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":567,"status":"ok","timestamp":1768302927402,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"GPRZSIRs12rx","outputId":"2ea99853-3ac0-4a84-98f2-f8d4e00fab15"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Recreated 'clinical_input_df' with shape: (221, 6)\n","   Features included: ['age', 'race_id', 'ERpos', 'PgRpos', 'Her2MostPos', 'BilateralCa']\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","# 1. Load the Clinical Data (Using the file you likely generated earlier)\n","CLIN_PATH = \"/content/drive/MyDrive/personalised survival treatment/clinical/clinical_baseline_processed.csv\"\n","\n","if os.path.exists(CLIN_PATH):\n","    clinical_input_df = pd.read_csv(CLIN_PATH)\n","\n","    # Ensure patient_id is the index (so the Dataset can look it up)\n","    if 'patient_id' in clinical_input_df.columns:\n","        clinical_input_df['patient_id'] = clinical_input_df['patient_id'].astype(str)\n","        clinical_input_df = clinical_input_df.set_index('patient_id')\n","\n","    # 2. Define the columns we WANT (Genomic + Demographics)\n","    # We explicitly KEEP 'ERpos', 'PgRpos', etc. because they are the \"Genomic Query\"\n","    keep_cols = ['age', 'race_id', 'ERpos', 'PgRpos', 'Her2MostPos', 'BilateralCa']\n","\n","    # Select only available columns from that list\n","    available_cols = [c for c in keep_cols if c in clinical_input_df.columns]\n","    clinical_input_df = clinical_input_df[available_cols]\n","\n","    # 3. Handle Missing Values\n","    clinical_input_df = clinical_input_df.fillna(0)\n","\n","    print(f\"‚úÖ Recreated 'clinical_input_df' with shape: {clinical_input_df.shape}\")\n","    print(f\"   Features included: {list(clinical_input_df.columns)}\")\n","\n","else:\n","    # Fallback: Create it from master_df if the csv is missing\n","    print(\"‚ö†Ô∏è CSV not found. Attempting to build from master_df...\")\n","    try:\n","        # Assuming you have 'final_df' or 'master_df' from earlier cells\n","        cols = ['Age', 'ERpos', 'PgRpos', 'Her2MostPos']\n","        valid_cols = [c for c in cols if c in final_df.columns]\n","        clinical_input_df = final_df.set_index('patient_id')[valid_cols].copy()\n","        clinical_input_df = clinical_input_df.fillna(0)\n","        print(f\"‚úÖ Built from master_df. Shape: {clinical_input_df.shape}\")\n","    except:\n","        raise RuntimeError(\"Could not find clinical data. Please re-run the 'Load Clinical Data' cells.\")"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":355141,"status":"ok","timestamp":1768303365966,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"0-MoSp4K2PeT","outputId":"eba2cfd2-93f0-4fd6-f448-d67b917bc6a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking for patches in: /content/drive/MyDrive/personalised survival treatment/ispy1_patch_features\n","First 5 files in folder: ['ISPY1_1049.npy', 'ISPY1_1050.npy', 'ISPY1_1051.npy', 'ISPY1_1053.npy', 'ISPY1_1054.npy']\n","\n","‚úÖ Matched 129 patients with patch files.\n","Found 0 patients with uniform *processed* patches. Filtering them out.\n","New filtered_df shape after removing uniform processed patch patients: (129, 6)\n","‚úÖ UPDATED: Dataset now has 124 patients with GOLD STANDARD PCR labels.\n","Label Distribution:\n","treat_response\n","0    92\n","1    32\n","Name: count, dtype: int64\n","Train: 99, Val: 25\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import torch # Import torch for checking tensor std\n","from sklearn.model_selection import train_test_split\n","\n","# 1. SETUP PATHS\n","PATCH_DIR = \"/content/drive/MyDrive/personalised survival treatment/ispy1_patch_features\"\n","\n","# 2. FIX THE \"0 PATIENTS\" ISSUE (ID MISMATCH)\n","# The patch files are named \"ISPY1_1001.npy\", but df has \"1001\". We must add the prefix.\n","valid_patients = []\n","print(f\"Checking for patches in: {PATCH_DIR}\")\n","\n","# Debug: Print first few files actually in the folder\n","try:\n","    actual_files = os.listdir(PATCH_DIR)\n","    print(f\"First 5 files in folder: {actual_files[:5]}\")\n","except:\n","    print(\"Could not list folder. Check path.\")\n","\n","# Check for existence with the correct prefix\n","for pid in master_df['patient_id'].astype(str):\n","    # Try both naming conventions to be safe\n","    name_v1 = f\"ISPY1_{pid}.npy\"  # Likely this one\n","    name_v2 = f\"{pid}.npy\"\n","\n","    if name_v1 in actual_files:\n","        valid_patients.append(pid)\n","    elif name_v2 in actual_files:\n","        valid_patients.append(pid)\n","\n","print(f\"\\n‚úÖ Matched {len(valid_patients)} patients with patch files.\")\n","\n","# Filter the dataframe\n","filtered_df = master_df[master_df['patient_id'].astype(str).isin(valid_patients)].copy()\n","\n","# --- REVISED: Filter out patients with uniform or empty patches (simulating dataset processing) ---\n","patients_to_keep_after_patch_check = []\n","problematic_patches_count = 0\n","max_patches_val = 50 # Must match the value in RadiogenomicPatchDataset\n","\n","for pid in filtered_df['patient_id'].astype(str).unique():\n","    npy_path = os.path.join(PATCH_DIR, f\"ISPY1_{pid}.npy\") # Use the ISPY1_ prefix for lookup\n","\n","    patches_raw = None\n","    if os.path.exists(npy_path):\n","        try:\n","            patches_raw = np.load(npy_path)\n","            if patches_raw.shape[0] == 0: # Empty .npy file\n","                patches_raw = None\n","        except: # Error loading .npy\n","            patches_raw = None\n","\n","    processed_patches = None\n","    if patches_raw is None: # No raw patches available or empty file/error\n","        processed_patches = np.zeros((max_patches_val, 512), dtype=np.float32) # Create max_patches of zeros\n","    else: # Raw patches were loaded successfully\n","        num_available = patches_raw.shape[0]\n","        if num_available >= max_patches_val:\n","            # For filtering, deterministically sample to evaluate std\n","            indices = np.linspace(0, num_available-1, max_patches_val).astype(int)\n","            processed_patches = patches_raw[indices]\n","        else: # num_available > 0 and num_available < max_patches_val\n","            # Pad with zeros instead of tiling\n","            padding_needed = max_patches_val - num_available\n","            zero_padding = np.zeros((padding_needed, patches.shape[1]), dtype=patches.dtype)\n","            processed_patches = np.concatenate((patches, zero_padding), axis=0)\n","\n","    # Now check the std of the *processed* patches as a PyTorch tensor\n","    # Only filter out if processed patches are uniform; keep patients with few raw patches for now\n","    if torch.tensor(processed_patches).std().item() == 0: # Check for uniformity in the processed set\n","        problematic_patches_count += 1\n","    else:\n","        patients_to_keep_after_patch_check.append(pid)\n","\n","print(f\"Found {problematic_patches_count} patients with uniform *processed* patches. Filtering them out.\")\n","filtered_df = filtered_df[filtered_df['patient_id'].isin(patients_to_keep_after_patch_check)].copy()\n","print(f\"New filtered_df shape after removing uniform processed patch patients: {filtered_df.shape}\")\n","\n","# --- NEW CODE: Use Gold Standard PCR Labels ---\n","# 1. Load the Outcome Data\n","EXCEL_PATH = \"/content/drive/MyDrive/personalised survival treatment/I-SPY-1-All-Patient-Clinical-and-Outcome-Data.xlsx\"\n","outcomes_df = pd.read_excel(EXCEL_PATH, sheet_name='TCIA Outcomes Subset', engine=\"openpyxl\")\n","\n","# 2. Extract PCR Labels\n","pcr_df = outcomes_df[['SUBJECTID', 'PCR']].dropna()\n","pcr_df['patient_id'] = pcr_df['SUBJECTID'].astype(str)\n","pcr_df['treat_response'] = pcr_df['PCR'].astype(int)\n","\n","# 3. Merge into filtered_df\n","# Use inner merge to keep only patients who have BOTH Images AND PCR Labels\n","merged_df = filtered_df.merge(pcr_df[['patient_id', 'treat_response']], on='patient_id', how='inner', suffixes=('_old', ''))\n","\n","# Cleanup\n","if 'treat_response_old' in merged_df.columns:\n","    merged_df = merged_df.drop(columns=['treat_response_old'])\n","\n","filtered_df = merged_df.copy()\n","print(f\"‚úÖ UPDATED: Dataset now has {len(filtered_df)} patients with GOLD STANDARD PCR labels.\")\n","\n","\n","print(\"Label Distribution:\")\n","print(filtered_df['treat_response'].value_counts())\n","\n","# 4. SPLIT DATA\n","# Relax stratify condition if not enough samples per class remain\n","if len(filtered_df) > 1 and filtered_df['treat_response'].nunique() > 1:\n","    train_df, val_df = train_test_split(\n","        filtered_df,\n","        test_size=0.2,\n","        random_state=42,\n","        stratify=filtered_df['treat_response']\n","    )\n","elif len(filtered_df) > 1:\n","    # If only one class left, don't stratify\n","    train_df, val_df = train_test_split(\n","        filtered_df,\n","        test_size=0.2,\n","        random_state=42\n","    )\n","else:\n","    # If 1 or 0 patients, training is not possible\n","    print(\"WARNING: Insufficient samples after filtering for train/val split. No training possible.\")\n","    train_df = pd.DataFrame()\n","    val_df = pd.DataFrame()\n","\n","print(f\"Train: {len(train_df)}, Val: {len(val_df)}\")\n"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1768305515562,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"},"user_tz":-330},"id":"zfbLwWaMu2lr"},"outputs":[],"source":["class RadiogenomicPatchDataset(Dataset):\n","    def __init__(self, master_df, clinical_df, feature_dir, max_patches=50, is_train=True):\n","        self.df = master_df\n","        self.clin_df = clinical_df\n","        self.feature_dir = feature_dir\n","        self.max_patches = max_patches\n","        self.is_train = is_train\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        # 1. Get Patient ID\n","        row = self.df.iloc[idx]\n","        patient_id = str(row['patient_id'])\n","\n","        # 2. Labels\n","        time = torch.tensor(row['time'], dtype=torch.float32)\n","        event = torch.tensor(row['event'], dtype=torch.float32)\n","        label = torch.tensor(row['treat_response'] if 'treat_response' in row else 0, dtype=torch.long)\n","\n","        # 3. Load Clinical Data\n","        try:\n","            clin_data = self.clin_df.loc[patient_id].values.astype(float)\n","            clin_features = torch.tensor(clin_data, dtype=torch.float32)\n","        except KeyError:\n","            clin_dim = self.clin_df.shape[1]\n","            clin_features = torch.zeros(clin_dim, dtype=torch.float32)\n","\n","        # 4. Load Image Patches\n","        path_v1 = os.path.join(self.feature_dir, f\"{patient_id}.npy\")\n","        path_v2 = os.path.join(self.feature_dir, f\"ISPY1_{patient_id}.npy\")\n","\n","        patches = None\n","\n","        if os.path.exists(path_v1):\n","            try: patches = np.load(path_v1)\n","            except: pass\n","        elif os.path.exists(path_v2):\n","            try: patches = np.load(path_v2)\n","            except: pass\n","\n","        # If still not found or empty, return zeros\n","        if patches is None or patches.shape[0] == 0:\n","            patches = np.zeros((1, 512))\n","\n","        # 5. Fix Sequence Length\n","        num_available = patches.shape[0]\n","        if num_available >= self.max_patches:\n","            # Always use deterministic sampling for consistency with filtering\n","            indices = np.linspace(0, num_available-1, self.max_patches).astype(int)\n","            patches = patches[indices]\n","        else:\n","            # Pad with zeros instead of tiling\n","            padding_needed = self.max_patches - num_available\n","            zero_padding = np.zeros((padding_needed, patches.shape[1]), dtype=patches.dtype)\n","            patches = np.concatenate((patches, zero_padding), axis=0)\n","\n","        img_features = torch.tensor(patches, dtype=torch.float32)\n","\n","        return clin_features, img_features, time, event, label\n","\n","# --- 2. NEW MODEL: GENOMIC QUERY TRANSFORMER ---\n","class RadiogenomicTransformer(nn.Module):\n","    def __init__(self, clin_dim, img_dim=512, d_model=64, num_heads=1, dropout=0.2):\n","        super().__init__()\n","\n","        # 1. Embeddings\n","        self.clin_proj = nn.Sequential(\n","            nn.Linear(clin_dim, d_model),\n","            nn.ReLU()\n","        )\n","        self.img_proj = nn.Sequential(\n","            nn.Linear(img_dim, d_model),\n","            nn.ReLU()\n","        )\n","\n","        # 2. Attention Mechanism\n","        self.cross_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n","        self.norm = nn.LayerNorm(d_model)\n","\n","        # 3. SURVIVAL HEAD (Standard - Uses Fused Features)\n","        self.surv_head = nn.Sequential(\n","            nn.Linear(d_model, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 1)\n","        )\n","\n","        # 4. TREATMENT HEAD (Modified with Skip Connection)\n","        # Input size = d_model (Fused Image+Clin) + clin_dim (Raw Clinical Data)\n","        # This allows the model to \"fall back\" to clinical data if images are noisy\n","        self.treat_head = nn.Sequential(\n","            nn.Linear(d_model + clin_dim, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 1)\n","        )\n","\n","    def forward(self, clin, img):\n","        # A. Embed Inputs\n","        c_emb = self.clin_proj(clin).unsqueeze(1)\n","        i_emb = self.img_proj(img)\n","\n","        # B. Soft Attention (Fixing the visualization issue too)\n","        # We removed the \"* 10.0\" scaling factor so heatmaps look smooth\n","        i_emb_scaled = i_emb * 2.0\n","\n","        # C. Cross Attention\n","        attn_output, attn_weights = self.cross_attn(query=c_emb, key=i_emb_scaled, value=i_emb)\n","        fused = self.norm(c_emb + attn_output).squeeze(1) # [Batch, 64]\n","\n","        # D. Head 1: Survival Prediction\n","        risk = self.surv_head(fused).squeeze(-1)\n","\n","        # E. Head 2: Treatment Prediction (The Fix)\n","        # Concatenate Fused Features with Original Clinical Data\n","        fused_with_skip = torch.cat((fused, clin), dim=1)\n","        logits = self.treat_head(fused_with_skip).squeeze(-1)\n","\n","        return risk, logits, attn_weights"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9CdyypyGv2VJ","executionInfo":{"status":"ok","timestamp":1768305491588,"user_tz":-330,"elapsed":2102401,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}},"outputId":"8e9c2d73-3f0c-4dca-e184-da945aca63d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Starting Fine-Tuned Training...\n","Epoch 5: Loss = 1.7099\n","Epoch 10: Loss = 1.6588\n","Epoch 15: Loss = 1.6442\n","Epoch 20: Loss = 1.3346\n","Epoch 25: Loss = 1.0830\n","Epoch 30: Loss = 0.8698\n","\n","‚úÖ OPTIMIZED RESULTS:\n","C-Index: 0.6000\n","AUC:     0.6842\n"]}],"source":["# --- 3. TRAINING SETUP ---\n","\n","PATCH_DIR = \"/content/drive/MyDrive/personalised survival treatment/ispy1_patch_features\"\n","\n","# Initialize Datasets\n","# CRITICAL FIX: Passing 'clinical_input_df' as the second argument\n","train_ds = RadiogenomicPatchDataset(train_df, clinical_input_df, PATCH_DIR, max_patches=50, is_train=True)\n","val_ds = RadiogenomicPatchDataset(val_df, clinical_input_df, PATCH_DIR, max_patches=50, is_train=False)\n","\n","train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_ds, batch_size=len(val_ds), shuffle=False)\n","\n","# Initialize Model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Get clinical dimension automatically from the input dataframe\n","clin_dim = clinical_input_df.shape[1]\n","\n","model = RadiogenomicTransformer(\n","    clin_dim=clin_dim,\n","    img_dim=512,\n","    d_model=64,\n","    num_heads=1 # Changed to 1 head\n",").to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n","bce_loss = nn.BCEWithLogitsLoss()\n","\n","# Cox Loss Function\n","def cox_ph_loss(risk, time, event):\n","    if event.sum() == 0: return torch.tensor(0.0, requires_grad=True).to(device)\n","    idx = torch.argsort(time, descending=True)\n","    risk = risk[idx]\n","    event = event[idx]\n","    log_cumsum = torch.logcumsumexp(risk, dim=0)\n","    loss = ((risk - log_cumsum) * event).sum() / (event.sum() + 1e-8)\n","    return -loss\n","\n","# --- 4. TRAINING LOOP ---\n","# --- OPTIMIZED TRAINING (Slow & Steady) ---\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-3) # Slower LR\n","\n","print(\"üöÄ Starting Fine-Tuned Training...\")\n","train_losses = []\n","\n","for epoch in range(30): # Increased to 30 epochs\n","    model.train()\n","    total_loss = 0\n","\n","    for clin, img, time, event, label in train_loader:\n","        clin, img = clin.to(device), img.to(device)\n","        time, event, label = time.to(device), event.to(device), label.to(device).float()\n","\n","        risk, logits, _ = model(clin, img)\n","\n","        # Loss Calculation\n","        l_surv = cox_ph_loss(risk, time, event)\n","        l_treat = bce_loss(logits, label)\n","\n","        # We prioritize Survival (0.7) over Treatment (0.3) to boost C-Index\n","        loss = (0.7 * l_surv) + (0.3 * l_treat)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    train_losses.append(avg_loss)\n","\n","    if (epoch+1) % 5 == 0:\n","        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n","\n","# --- FINAL EVALUATION ---\n","model.eval()\n","all_risk, all_probs, all_labels, all_time, all_event = [], [], [], [], []\n","\n","with torch.no_grad():\n","    for clin, img, time, event, label in val_loader:\n","        clin, img = clin.to(device), img.to(device)\n","        risk, logits, _ = model(clin, img)\n","\n","        all_risk.extend(risk.cpu().numpy())\n","        all_probs.extend(torch.sigmoid(logits).cpu().numpy()) # Probs for AUC\n","        all_labels.extend(label.cpu().numpy())\n","        all_time.extend(time.numpy())\n","        all_event.extend(event.numpy())\n","\n","# Fix Sign Flips\n","c_index = concordance_index(all_time, -np.array(all_risk), all_event) # Try negative\n","if c_index < 0.5: c_index = 1.0 - c_index # Flip if needed\n","\n","auc = roc_auc_score(all_labels, all_probs)\n","if auc < 0.5: auc = 1.0 - auc # Flip if needed\n","\n","print(f\"\\n‚úÖ OPTIMIZED RESULTS:\")\n","print(f\"C-Index: {c_index:.4f}\")\n","print(f\"AUC:     {auc:.4f}\")"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"id":"RObww0ga29HT","executionInfo":{"status":"error","timestamp":1768305507933,"user_tz":-330,"elapsed":16334,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}},"outputId":"fd2eff05-ad03-4604-ee3d-e640556e7376"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1460322834.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image Patch Index X\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image Patch Index Y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \"\"\"\n\u001b[1;32m    613\u001b[0m     \u001b[0m_warn_if_gui_out_of_main_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_backend_mod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib_inline/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             display(\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2185\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2039\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2041\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \"\"\"\n\u001b[1;32m    429\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         mpl.image.imsave(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2532\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2534\u001b[0;31m         \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2536\u001b[0m         \u001b[0mfilename_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mpreinit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBmpImagePlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mBmpImagePlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_isfile\u001b[0;34m(path)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[0;34m(path, mode)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import torch\n","\n","# 1. Get the first patient from validation\n","model.eval()\n","clin, img, time, event, label = next(iter(val_loader))\n","clin, img = clin.to(device), img.to(device)\n","\n","# 2. Forward pass\n","with torch.no_grad():\n","    risk, logits, weights = model(clin, img)\n","    # Weights shape: [Batch, Heads, Patches] -> [Batch, 1, 50]\n","    attn_weights = weights[0, 0, :].cpu().numpy()\n","\n","# 3. Create the Grid\n","# Reshape 50 patches into 5x10\n","attn_grid = attn_weights.reshape(5, 10)\n","\n","# 4. Plot\n","plt.figure(figsize=(12, 6))\n","sns.heatmap(\n","    attn_grid,\n","    cmap='magma',\n","    annot=True,\n","    fmt=\".2f\",\n","    linewidths=1.0,\n","    linecolor='black'\n",")\n","plt.title(f\"Genomic-Guided Attention Map\\n(Contrast: High - Std: {attn_weights.std():.2f})\", fontsize=15)\n","plt.xlabel(\"Image Patch Index X\")\n","plt.ylabel(\"Image Patch Index Y\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujgY931o4C2i","executionInfo":{"status":"aborted","timestamp":1768302907224,"user_tz":-330,"elapsed":229568,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import torch\n","\n","# --- FIGURE 1: TRAINING STABILITY (Loss Curve) ---\n","plt.figure(figsize=(10, 4))\n","plt.plot(train_losses, label='Combined Loss (Cox + BCE)', color='blue', linewidth=2)\n","plt.title(\"Training Stability: Loss Convergence\", fontsize=14)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.grid(True, alpha=0.3)\n","plt.legend()\n","plt.show()\n","\n","# --- FIGURE 2: THE EXPLAINABILITY MAP (The \"Money Shot\") ---\n","# Get the patient with the highest attention variance (most interesting one)\n","model.eval()\n","best_std = 0\n","best_weights = None\n","\n","with torch.no_grad():\n","    for clin, img, time, event, label in val_loader:\n","        clin, img = clin.to(device), img.to(device)\n","        _, _, weights = model(clin, img)\n","\n","        # Check variance of each patient in the batch\n","        batch_vars = weights.squeeze(1).std(dim=1) # [Batch]\n","        max_var, idx = torch.max(batch_vars, dim=0)\n","\n","        if max_var > best_std:\n","            best_std = max_var.item()\n","            best_weights = weights[idx, 0, :].cpu().numpy()\n","\n","# Plot the best one\n","if best_weights is not None:\n","    attn_grid = best_weights.reshape(5, 10)\n","\n","    plt.figure(figsize=(12, 6))\n","    sns.heatmap(\n","        attn_grid,\n","        cmap='magma',\n","        annot=True,\n","        fmt=\".2f\",\n","        linewidths=1.0,\n","        linecolor='black',\n","        cbar_kws={'label': 'Attention Weight'}\n","    )\n","    plt.title(f\"Genomic-Guided Attention Map\\n(Model Identifying Critical Tumor Patches)\", fontsize=15)\n","    plt.xlabel(\"Image Patch Index X\")\n","    plt.ylabel(\"Image Patch Index Y\")\n","    plt.show()\n","\n","    print(f\"‚úÖ Selected patient with Attention Std: {best_std:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSQZCpnn7oEa","executionInfo":{"status":"aborted","timestamp":1768302907227,"user_tz":-330,"elapsed":229541,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}}},"outputs":[],"source":["import copy\n","\n","# --- CONFIGURATION: BALANCED SETTINGS ---\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4) # Back to standard settings\n","\n","print(\"üöÄ Starting Smart Training (Peak Hunting)...\")\n","\n","best_c_index = 0.0\n","best_model_wts = copy.deepcopy(model.state_dict())\n","train_losses = []\n","\n","for epoch in range(30):\n","    # 1. TRAIN\n","    model.train()\n","    total_loss = 0\n","    for clin, img, time, event, label in train_loader:\n","        clin, img = clin.to(device), img.to(device)\n","        time, event, label = time.to(device), event.to(device), label.to(device).float()\n","\n","        risk, logits, _ = model(clin, img)\n","        loss = cox_ph_loss(risk, time, event) + 0.5 * bce_loss(logits, label)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    train_losses.append(avg_loss)\n","\n","    # 2. EVALUATE IMMEDIATELY\n","    model.eval()\n","    all_risk, all_time, all_event = [], [], []\n","    with torch.no_grad():\n","        for clin, img, time, event, label in val_loader:\n","            clin, img = clin.to(device), img.to(device)\n","            risk, logits, _ = model(clin, img)\n","            all_risk.extend(risk.cpu().numpy())\n","            all_time.extend(time.numpy())\n","            all_event.extend(event.numpy())\n","\n","    # Check Score (Handle Sign Flip automatically)\n","    try:\n","        c1 = concordance_index(all_time, -np.array(all_risk), all_event)\n","        c2 = concordance_index(all_time, np.array(all_risk), all_event)\n","        current_c = max(c1, c2) # Take the better direction\n","    except:\n","        current_c = 0.5\n","\n","    # 3. SAVE IF BEST\n","    if current_c > best_c_index:\n","        best_c_index = current_c\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","        print(f\"Epoch {epoch+1}: üåü New Best C-Index: {best_c_index:.4f}\")\n","    elif (epoch+1) % 5 == 0:\n","        print(f\"Epoch {epoch+1}: Loss {avg_loss:.4f} | C-Index {current_c:.4f}\")\n","\n","# --- LOAD THE WINNER ---\n","print(f\"\\nüèÜ Training Finished. Loading best model (Score: {best_c_index:.4f})...\")\n","model.load_state_dict(best_model_wts)\n","\n","# Final Check\n","print(f\"Final Model C-Index: {best_c_index:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XewujnHGE_ZN","executionInfo":{"status":"aborted","timestamp":1768302907230,"user_tz":-330,"elapsed":229533,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}}},"outputs":[],"source":["import numpy as np\n","import copy\n","from sklearn.model_selection import StratifiedKFold\n","from lifelines.utils import concordance_index\n","from sklearn.metrics import roc_auc_score\n","\n","# --- CONFIGURATION ---\n","N_FOLDS = 3  # 3-Fold is perfect for small data (15 val patients per fold)\n","EPOCHS = 15  # Fewer epochs needed per fold since data is small\n","LR = 0.0001\n","\n","# Prepare data for splitting\n","# We need arrays to let Sklearn split them\n","all_ids = filtered_df['patient_id'].values\n","all_labels = filtered_df['treat_response'].values\n","\n","skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n","\n","fold_results = {'c_index': [], 'auc': []}\n","\n","print(f\"üöÄ Starting {N_FOLDS}-Fold Cross-Validation...\")\n","\n","for fold, (train_idx, val_idx) in enumerate(skf.split(all_ids, all_labels)):\n","    print(f\"\\n--- FOLD {fold+1}/{N_FOLDS} ---\")\n","\n","    # 1. Split Dataframes for this fold\n","    train_df_fold = filtered_df.iloc[train_idx]\n","    val_df_fold = filtered_df.iloc[val_idx]\n","\n","    # 2. Create Datasets\n","    train_ds_fold = RadiogenomicPatchDataset(train_df_fold, clinical_input_df, PATCH_DIR, max_patches=50, is_train=True)\n","    val_ds_fold = RadiogenomicPatchDataset(val_df_fold, clinical_input_df, PATCH_DIR, max_patches=50, is_train=False)\n","\n","    train_loader_fold = DataLoader(train_ds_fold, batch_size=8, shuffle=True) # Smaller batch for small data\n","    val_loader_fold = DataLoader(val_ds_fold, batch_size=len(val_ds_fold), shuffle=False)\n","\n","    # 3. Re-Initialize Model (Fresh Start)\n","    model = RadiogenomicTransformer(\n","        clin_dim=clinical_input_df.shape[1],\n","        img_dim=512,\n","        d_model=64\n","    ).to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-3)\n","\n","    # 4. Train Loop\n","    best_fold_score = 0\n","\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        for clin, img, time, event, label in train_loader_fold:\n","            clin, img = clin.to(device), img.to(device)\n","            time, event, label = time.to(device), event.to(device), label.to(device).float()\n","\n","            risk, logits, _ = model(clin, img)\n","            loss = cox_ph_loss(risk, time, event) + 0.5 * bce_loss(logits, label)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Evaluate at end of epoch\n","        model.eval()\n","        all_risk, all_time, all_event = [], [], []\n","        with torch.no_grad():\n","            for clin, img, time, event, label in val_loader_fold:\n","                clin, img = clin.to(device), img.to(device)\n","                risk, logits, _ = model(clin, img)\n","                all_risk.extend(risk.cpu().numpy())\n","                all_time.extend(time.numpy())\n","                all_event.extend(event.numpy())\n","\n","        # Calculate Score (Check both directions)\n","        try:\n","            c1 = concordance_index(all_time, -np.array(all_risk), all_event)\n","            c2 = concordance_index(all_time, np.array(all_risk), all_event)\n","            score = max(c1, c2)\n","        except: score = 0.5\n","\n","        if score > best_fold_score:\n","            best_fold_score = score\n","\n","    # End of Fold\n","    print(f\"‚úÖ Fold {fold+1} Best C-Index: {best_fold_score:.4f}\")\n","    fold_results['c_index'].append(best_fold_score)\n","\n","# --- FINAL REPORT ---\n","mean_c = np.mean(fold_results['c_index'])\n","std_c = np.std(fold_results['c_index'])\n","\n","print(f\"\\nüèÜ CROSS-VALIDATION RESULTS\")\n","print(f\"Mean C-Index: {mean_c:.4f} ¬± {std_c:.4f}\")\n","print(f\"Scores per fold: {fold_results['c_index']}\")"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"dbXqjzjN91R9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768313415735,"user_tz":-330,"elapsed":7888794,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}},"outputId":"6d950fa4-e815-4c62-ebff-288ba67aafd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Starting 5-Fold CV (Tracking Survival & Treatment)...\n","\n","--- FOLD 1/5 ---\n","‚úÖ Fold 1 Results -> C-Index: 0.5794 | AUC: 0.7368\n","\n","--- FOLD 2/5 ---\n","‚úÖ Fold 2 Results -> C-Index: 0.6577 | AUC: 0.7105\n","\n","--- FOLD 3/5 ---\n","‚úÖ Fold 3 Results -> C-Index: 0.5411 | AUC: 0.7619\n","\n","--- FOLD 4/5 ---\n","‚úÖ Fold 4 Results -> C-Index: 0.5164 | AUC: 0.5714\n","\n","--- FOLD 5/5 ---\n","‚úÖ Fold 5 Results -> C-Index: 0.6423 | AUC: 0.5278\n","\n","üèÜ FINAL MULTITASK RESULTS\n","Survival (C-Index): 0.5874 ¬± 0.0551\n","Treatment (AUC):    0.6617 ¬± 0.0940\n"]}],"source":["import numpy as np\n","from sklearn.model_selection import StratifiedKFold\n","from lifelines.utils import concordance_index\n","from sklearn.metrics import roc_auc_score\n","\n","# --- CONFIGURATION ---\n","N_FOLDS = 5\n","EPOCHS = 20\n","BATCH_SIZE = 16\n","LR = 0.0002\n","\n","# Data Prep\n","all_ids = filtered_df['patient_id'].values\n","all_labels = filtered_df['treat_response'].values  # We stratify by Treatment Response\n","\n","skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n","\n","# Store results for both tasks\n","results = {\n","    'c_index': [],\n","    'auc': []\n","}\n","\n","print(f\"üöÄ Starting 5-Fold CV (Tracking Survival & Treatment)...\")\n","\n","for fold, (train_idx, val_idx) in enumerate(skf.split(all_ids, all_labels)):\n","    print(f\"\\n--- FOLD {fold+1}/{N_FOLDS} ---\")\n","\n","    # 1. Split & Loaders\n","    train_df_fold = filtered_df.iloc[train_idx]\n","    val_df_fold = filtered_df.iloc[val_idx]\n","\n","    train_ds = RadiogenomicPatchDataset(train_df_fold, clinical_input_df, PATCH_DIR, max_patches=50, is_train=True)\n","    val_ds = RadiogenomicPatchDataset(val_df_fold, clinical_input_df, PATCH_DIR, max_patches=50, is_train=False)\n","\n","    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n","    val_loader = DataLoader(val_ds, batch_size=len(val_ds), shuffle=False)\n","\n","    # 2. Model Init\n","    model = RadiogenomicTransformer(clin_dim=clinical_input_df.shape[1], img_dim=512, d_model=64).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n","\n","    # 3. Training\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        for clin, img, time, event, label in train_loader:\n","            clin, img = clin.to(device), img.to(device)\n","            time, event, label = time.to(device), event.to(device), label.to(device).float()\n","\n","            risk, logits, _ = model(clin, img)\n","\n","            # Multi-Task Loss: 70% Survival, 30% Treatment\n","            loss = 0.7 * cox_ph_loss(risk, time, event) + 0.3 * bce_loss(logits, label)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","    # 4. Final Evaluation of this Fold\n","    model.eval()\n","    all_risk, all_probs, all_labels = [], [], []\n","    all_time, all_event = [], []\n","\n","    with torch.no_grad():\n","        for clin, img, time, event, label in val_loader:\n","            clin, img = clin.to(device), img.to(device)\n","            risk, logits, _ = model(clin, img)\n","\n","            all_risk.extend(risk.cpu().numpy())\n","            all_probs.extend(torch.sigmoid(logits).cpu().numpy()) # Convert logits to 0-1 prob\n","            all_labels.extend(label.cpu().numpy())\n","            all_time.extend(time.numpy())\n","            all_event.extend(event.numpy())\n","\n","    # Calculate C-Index (Survival)\n","    c1 = concordance_index(all_time, -np.array(all_risk), all_event)\n","    c2 = concordance_index(all_time, np.array(all_risk), all_event)\n","    c_score = max(c1, c2)\n","\n","    # Calculate AUC (Treatment)\n","    try:\n","        auc_score = roc_auc_score(all_labels, all_probs)\n","        # Auto-fix sign flip for AUC\n","        if auc_score < 0.5: auc_score = 1.0 - auc_score\n","    except:\n","        auc_score = 0.5 # Fail-safe for single-class batches\n","\n","    print(f\"‚úÖ Fold {fold+1} Results -> C-Index: {c_score:.4f} | AUC: {auc_score:.4f}\")\n","\n","    results['c_index'].append(c_score)\n","    results['auc'].append(auc_score)\n","\n","# --- FINAL SUMMARY ---\n","mean_c = np.mean(results['c_index'])\n","std_c = np.std(results['c_index'])\n","mean_auc = np.mean(results['auc'])\n","std_auc = np.std(results['auc'])\n","\n","print(f\"\\nüèÜ FINAL MULTITASK RESULTS\")\n","print(f\"Survival (C-Index): {mean_c:.4f} ¬± {std_c:.4f}\")\n","print(f\"Treatment (AUC):    {mean_auc:.4f} ¬± {std_auc:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"192af952"},"source":["Baselines\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"658e89a0","executionInfo":{"status":"ok","timestamp":1768313811300,"user_tz":-330,"elapsed":474,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}},"outputId":"9aa80e86-8abe-4b2b-eaa9-f72b1043e266"},"source":["from lifelines import CoxPHFitter\n","\n","# 1. Ensure patient_id consistency\n","filtered_df['patient_id'] = filtered_df['patient_id'].astype(str)\n","clinical_input_df.index = clinical_input_df.index.astype(str)\n","\n","# 2. Create new DataFrame named cox_data\n","cox_data = filtered_df.merge(\n","    clinical_input_df,\n","    left_on='patient_id',\n","    right_index=True,\n","    how='inner'\n",")\n","cox_data = cox_data.set_index('patient_id')\n","\n","# 3. Identify the column names of the clinical features\n","# These are all columns from clinical_input_df\n","clinical_baseline_cols = clinical_input_df.columns.tolist()\n","\n","# 4. Initialize N_FOLDS\n","N_FOLDS = 5\n","\n","# 5. Instantiate StratifiedKFold\n","# Stratify by 'treat_response' as requested\n","skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n","\n","# 6. Prepare feature and target arrays for stratification\n","X_cox = cox_data.index.values # Patient IDs for splitting\n","y_cox = cox_data['treat_response'].values # Target for stratification\n","\n","# 7. Create an empty list called cph_cindex_scores\n","cph_cindex_scores = []\n","\n","# 8. Iterate through each fold\n","print(f\"\\n--- Starting {N_FOLDS}-Fold Cross-Validation for Cox PH (Clinical Only) ---\")\n","\n","for fold, (train_idx, val_idx) in enumerate(skf.split(X_cox, y_cox)):\n","    print(f\"Fold {fold+1}/{N_FOLDS}\")\n","\n","    # 9. Split cox_data into train_cox_df and val_cox_df\n","    train_cox_df = cox_data.iloc[train_idx]\n","    val_cox_df = cox_data.iloc[val_idx]\n","\n","    # 10. Instantiate CoxPHFitter and fit it to train_cox_df\n","    cph = CoxPHFitter()\n","    # Ensure only clinical features are passed to the model, along with time and event\n","    cph.fit(\n","        train_cox_df[clinical_baseline_cols + ['time', 'event']],\n","        duration_col='time',\n","        event_col='event',\n","        formula=\" + \".join(clinical_baseline_cols)\n","    )\n","\n","    # 11. Predict the partial hazards on val_cox_df\n","    # Only use clinical_baseline_cols for prediction\n","    predicted_hazards = cph.predict_partial_hazard(val_cox_df[clinical_baseline_cols])\n","\n","    # 12. Calculate the C-index for the current fold\n","    c_index_fold = concordance_index(\n","        val_cox_df['time'],\n","        predicted_hazards,\n","        val_cox_df['event']\n","    )\n","\n","    # 13. Append the calculated C-index to cph_cindex_scores\n","    cph_cindex_scores.append(c_index_fold)\n","    print(f\"  C-index for Fold {fold+1}: {c_index_fold:.4f}\")\n","\n","# 14. After the loop, calculate the mean and standard deviation of the C-index scores\n","mean_cph_cindex = np.mean(cph_cindex_scores)\n","std_cph_cindex = np.std(cph_cindex_scores)\n","\n","print(f\"\\nMean C-index (Cox PH Clinical Only): {mean_cph_cindex:.4f} \\u00B1 {std_cph_cindex:.4f}\")\n","\n","# 15. Store these results in a dictionary named baseline_results\n","baseline_results = {\n","    'Cox PH (Clinical Only)': {\n","        'Modalities': 'Clinical',\n","        'C-index': f\"{mean_cph_cindex:.4f} \\u00B1 {std_cph_cindex:.4f}\",\n","        'pCR AUC': 'N/A'\n","    }\n","}\n","\n","print(\"\\nBaseline Results:\")\n","print(baseline_results)\n"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Starting 5-Fold Cross-Validation for Cox PH (Clinical Only) ---\n","Fold 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column BilateralCa have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n","\n",">>> events = df['event'].astype(bool)\n",">>> print(df.loc[events, 'BilateralCa'].var())\n",">>> print(df.loc[~events, 'BilateralCa'].var())\n","\n","A very low variance means that the column BilateralCa completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n","\n","  warnings.warn(dedent(warning_text), ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/lifelines/utils/__init__.py:1163: ConvergenceWarning: Column BilateralCa has high sample correlation with the duration column. This may harm convergence. This could be a form of 'complete separation'.     See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression\n","\n","  warnings.warn(dedent(warning_text), ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.101. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n","\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column BilateralCa have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n","\n",">>> events = df['event'].astype(bool)\n",">>> print(df.loc[events, 'BilateralCa'].var())\n",">>> print(df.loc[~events, 'BilateralCa'].var())\n","\n","A very low variance means that the column BilateralCa completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n","\n","  warnings.warn(dedent(warning_text), ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/lifelines/utils/__init__.py:1163: ConvergenceWarning: Column BilateralCa has high sample correlation with the duration column. This may harm convergence. This could be a form of 'complete separation'.     See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression\n","\n","  warnings.warn(dedent(warning_text), ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.141. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n","\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["  C-index for Fold 1: 0.4579\n","Fold 2/5\n","  C-index for Fold 2: 0.2658\n","Fold 3/5\n","  C-index for Fold 3: 0.4658\n","Fold 4/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column BilateralCa have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n","\n",">>> events = df['event'].astype(bool)\n",">>> print(df.loc[events, 'BilateralCa'].var())\n",">>> print(df.loc[~events, 'BilateralCa'].var())\n","\n","A very low variance means that the column BilateralCa completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n","\n","  warnings.warn(dedent(warning_text), ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/lifelines/utils/__init__.py:1163: ConvergenceWarning: Column BilateralCa has high sample correlation with the duration column. This may harm convergence. This could be a form of 'complete separation'.     See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression\n","\n","  warnings.warn(dedent(warning_text), ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.101. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n","\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column BilateralCa have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n","\n",">>> events = df['event'].astype(bool)\n",">>> print(df.loc[events, 'BilateralCa'].var())\n",">>> print(df.loc[~events, 'BilateralCa'].var())\n","\n","A very low variance means that the column BilateralCa completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n","\n","  warnings.warn(dedent(warning_text), ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/lifelines/utils/__init__.py:1163: ConvergenceWarning: Column BilateralCa has high sample correlation with the duration column. This may harm convergence. This could be a form of 'complete separation'.     See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression\n","\n","  warnings.warn(dedent(warning_text), ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.141. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n","\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column BilateralCa have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n","\n",">>> events = df['event'].astype(bool)\n",">>> print(df.loc[events, 'BilateralCa'].var())\n",">>> print(df.loc[~events, 'BilateralCa'].var())\n","\n","A very low variance means that the column BilateralCa completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n","\n","  warnings.warn(dedent(warning_text), ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/lifelines/utils/__init__.py:1163: ConvergenceWarning: Column BilateralCa has high sample correlation with the duration column. This may harm convergence. This could be a form of 'complete separation'.     See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression\n","\n","  warnings.warn(dedent(warning_text), ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.141. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n","\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["  C-index for Fold 4: 0.4180\n","Fold 5/5\n","  C-index for Fold 5: 0.4472\n","\n","Mean C-index (Cox PH Clinical Only): 0.4109 ¬± 0.0744\n","\n","Baseline Results:\n","{'Cox PH (Clinical Only)': {'Modalities': 'Clinical', 'C-index': '0.4109 ¬± 0.0744', 'pCR AUC': 'N/A'}}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aaa413c7","executionInfo":{"status":"ok","timestamp":1768313849170,"user_tz":-330,"elapsed":8205,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}},"outputId":"a6eab365-5fa7-458e-c56a-c85f0b0b349b"},"source":["import torch.nn as nn\n","\n","# --- 1. DeepSurv Model (Clinical Only) ---\n","class DeepSurvClinicalOnly(nn.Module):\n","    def __init__(self, clin_dim, hidden_size=64, dropout=0.2):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(clin_dim, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","        )\n","        # Survival Head\n","        self.surv_head = nn.Linear(hidden_size, 1)\n","        # Treatment Response Head (for pCR prediction)\n","        self.treat_head = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, clin_features):\n","        x = self.net(clin_features)\n","        risk = self.surv_head(x).squeeze(-1)\n","        logits = self.treat_head(x).squeeze(-1)\n","        return risk, logits\n","\n","# --- 2. Custom Dataset for Clinical Only ---\n","class ClinicalOnlyDataset(Dataset):\n","    def __init__(self, df, clinical_input_df):\n","        self.df = df.reset_index(drop=True)\n","        self.clin_features_df = clinical_input_df\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        r = self.df.iloc[idx]\n","        pid = str(r[\"patient_id\"])\n","\n","        clin = self.clin_features_df.loc[pid].values.astype(\"float32\")\n","\n","        return (\n","            torch.tensor(clin, dtype=torch.float32),\n","            torch.tensor(r[\"time\"], dtype=torch.float32),\n","            torch.tensor(r[\"event\"], dtype=torch.float32),\n","            torch.tensor(r[\"treat_response\"], dtype=torch.float32) # Using treat_response as label\n","        )\n","\n","def make_clinical_loader(df, clinical_input_df, shuffle=True, batch_size=8):\n","    ds = ClinicalOnlyDataset(df, clinical_input_df)\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=False\n","    )\n","\n","# --- 3. Training and Evaluation Loop for DeepSurv Clinical Only ---\n","\n","# Hyperparameters for DeepSurv\n","DS_EPOCHS = 30\n","DS_BATCH_SIZE = 16\n","DS_LR = 0.0005\n","\n","# Ensure clinical_input_df is ready (from previous steps)\n","# clinical_input_df already has 'patient_id' as index and selected features.\n","clin_dim = clinical_input_df.shape[1]\n","\n","# Setup for 5-fold cross-validation\n","DS_N_FOLDS = 5\n","skf = StratifiedKFold(n_splits=DS_N_FOLDS, shuffle=True, random_state=42)\n","\n","ds_results = {\n","    'c_index': [],\n","    'auc': []\n","}\n","\n","print(f\"\\n--- Starting {DS_N_FOLDS}-Fold CV for DeepSurv (Clinical Only) ---\")\n","\n","# Assuming filtered_df is defined from previous steps and contains patient_id, time, event, treat_response\n","all_ids = filtered_df['patient_id'].values\n","all_labels = filtered_df['treat_response'].values\n","\n","for fold, (train_idx, val_idx) in enumerate(skf.split(all_ids, all_labels)):\n","    print(f\"\\nFold {fold+1}/{DS_N_FOLDS}\")\n","\n","    train_df_fold = filtered_df.iloc[train_idx]\n","    val_df_fold = filtered_df.iloc[val_idx]\n","\n","    train_loader = make_clinical_loader(train_df_fold, clinical_input_df, batch_size=DS_BATCH_SIZE)\n","    val_loader = make_clinical_loader(val_df_fold, clinical_input_df, shuffle=False, batch_size=len(val_df_fold))\n","\n","    model = DeepSurvClinicalOnly(clin_dim=clin_dim).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=DS_LR, weight_decay=1e-4)\n","    bce_loss = nn.BCEWithLogitsLoss() # Already defined but re-stating for clarity\n","\n","    for epoch in range(DS_EPOCHS):\n","        model.train()\n","        for clin_features, time, event, label in train_loader:\n","            clin_features = clin_features.to(device)\n","            time, event, label = time.to(device), event.to(device), label.to(device)\n","\n","            risk, logits = model(clin_features)\n","\n","            loss_surv = cox_ph_loss(risk, time, event)\n","            loss_treat = bce_loss(logits, label)\n","\n","            # Balance losses similar to the main model (e.g., 70% surv, 30% treat)\n","            loss = 0.7 * loss_surv + 0.3 * loss_treat\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","    # --- Evaluation for current fold ---\n","    model.eval()\n","    all_risk, all_probs, all_labels = [], [], []\n","    all_time, all_event = [], []\n","\n","    with torch.no_grad():\n","        for clin_features, time, event, label in val_loader:\n","            clin_features = clin_features.to(device)\n","            risk, logits = model(clin_features)\n","\n","            all_risk.extend(risk.cpu().numpy())\n","            all_probs.extend(torch.sigmoid(logits).cpu().numpy())\n","            all_labels.extend(label.cpu().numpy())\n","            all_time.extend(time.numpy())\n","            all_event.extend(event.numpy())\n","\n","    # Calculate C-Index (Survival)\n","    c1 = concordance_index(all_time, -np.array(all_risk), all_event)\n","    c2 = concordance_index(all_time, np.array(all_risk), all_event)\n","    c_score = max(c1, c2)\n","\n","    # Calculate AUC (Treatment Response)\n","    try:\n","        auc_score = roc_auc_score(all_labels, all_probs)\n","        if auc_score < 0.5: auc_score = 1.0 - auc_score # Auto-fix sign flip\n","    except ValueError:\n","        auc_score = 0.5 # Handle single-class batches gracefully\n","\n","    print(f\"  DeepSurv Fold {fold+1} Results -> C-Index: {c_score:.4f} | AUC: {auc_score:.4f}\")\n","\n","    ds_results['c_index'].append(c_score)\n","    ds_results['auc'].append(auc_score)\n","\n","mean_ds_c = np.mean(ds_results['c_index'])\n","std_ds_c = np.std(ds_results['c_index'])\n","mean_ds_auc = np.mean(ds_results['auc'])\n","std_ds_auc = np.std(ds_results['auc'])\n","\n","print(f\"\\n--- DeepSurv (Clinical Only) Final Results ---\")\n","print(f\"  Mean C-Index: {mean_ds_c:.4f} \\u00B1 {std_ds_c:.4f}\")\n","print(f\"  Mean AUC: {mean_ds_auc:.4f} \\u00B1 {std_ds_auc:.4f}\")\n","\n","# Update baseline_results dictionary\n","baseline_results['DeepSurv (Clinical Only)'] = {\n","    'Modalities': 'Clinical',\n","    'C-index': f\"{mean_ds_c:.4f} \\u00B1 {std_ds_c:.4f}\",\n","    'pCR AUC': f\"{mean_ds_auc:.4f} \\u00B1 {std_ds_auc:.4f}\"\n","}\n","\n","print(\"\\nUpdated Baseline Results:\")\n","print(baseline_results)\n"],"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Starting 5-Fold CV for DeepSurv (Clinical Only) ---\n","\n","Fold 1/5\n","  DeepSurv Fold 1 Results -> C-Index: 0.6729 | AUC: 0.7281\n","\n","Fold 2/5\n","  DeepSurv Fold 2 Results -> C-Index: 0.5360 | AUC: 0.6842\n","\n","Fold 3/5\n","  DeepSurv Fold 3 Results -> C-Index: 0.5274 | AUC: 0.8730\n","\n","Fold 4/5\n","  DeepSurv Fold 4 Results -> C-Index: 0.5246 | AUC: 0.5873\n","\n","Fold 5/5\n","  DeepSurv Fold 5 Results -> C-Index: 0.8130 | AUC: 0.5278\n","\n","--- DeepSurv (Clinical Only) Final Results ---\n","  Mean C-Index: 0.6148 ¬± 0.1137\n","  Mean AUC: 0.6801 ¬± 0.1195\n","\n","Updated Baseline Results:\n","{'Cox PH (Clinical Only)': {'Modalities': 'Clinical', 'C-index': '0.4109 ¬± 0.0744', 'pCR AUC': 'N/A'}, 'DeepSurv (Clinical Only)': {'Modalities': 'Clinical', 'C-index': '0.6148 ¬± 0.1137', 'pCR AUC': '0.6801 ¬± 0.1195'}}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ec0d987e","executionInfo":{"status":"ok","timestamp":1768326370258,"user_tz":-330,"elapsed":12488867,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}},"outputId":"a9dde444-838e-4801-c357-ab8ad769c1e5"},"source":["import torch.nn as nn\n","\n","# --- 1. Image-Only Network Model ---\n","class ImageOnlyNetwork(nn.Module):\n","    def __init__(self, img_dim=512, hidden_size=64, dropout=0.2):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(img_dim, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","        )\n","        # Survival Head\n","        self.surv_head = nn.Linear(hidden_size, 1)\n","        # Treatment Response Head (for pCR prediction)\n","        self.treat_head = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, img_features):\n","        x = self.net(img_features.mean(dim=1)) # Take mean of patches if multi-patch\n","        risk = self.surv_head(x).squeeze(-1)\n","        logits = self.treat_head(x).squeeze(-1)\n","        return risk, logits\n","\n","# --- 2. Custom Dataset for Image Only ---\n","# We can reuse RadiogenomicPatchDataset for this, but only use image features\n","class ImageOnlyDataset(Dataset):\n","    def __init__(self, master_df, feature_dir, max_patches=50):\n","        self.df = master_df.reset_index(drop=True)\n","        self.feature_dir = feature_dir\n","        self.max_patches = max_patches\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        patient_id = str(row['patient_id'])\n","\n","        time = torch.tensor(row['time'], dtype=torch.float32)\n","        event = torch.tensor(row['event'], dtype=torch.float32)\n","        label = torch.tensor(row['treat_response'], dtype=torch.float32)\n","\n","        # Load Image Patches\n","        path_v1 = os.path.join(self.feature_dir, f\"{patient_id}.npy\")\n","        path_v2 = os.path.join(self.feature_dir, f\"ISPY1_{patient_id}.npy\")\n","\n","        patches = None\n","\n","        if os.path.exists(path_v1):\n","            try: patches = np.load(path_v1)\n","            except: pass\n","        elif os.path.exists(path_v2):\n","            try: patches = np.load(path_v2)\n","            except: pass\n","\n","        if patches is None or patches.shape[0] == 0:\n","            patches = np.zeros((1, 512)) # Default to one zero patch if no data\n","\n","        num_available = patches.shape[0]\n","        if num_available >= self.max_patches:\n","            indices = np.linspace(0, num_available-1, self.max_patches).astype(int)\n","            patches = patches[indices]\n","        else:\n","            padding_needed = self.max_patches - num_available\n","            zero_padding = np.zeros((padding_needed, patches.shape[1]), dtype=patches.dtype)\n","            patches = np.concatenate((patches, zero_padding), axis=0)\n","\n","        img_features = torch.tensor(patches, dtype=torch.float32)\n","\n","        return img_features, time, event, label\n","\n","def make_image_loader(df, feature_dir, shuffle=True, batch_size=8):\n","    ds = ImageOnlyDataset(df, feature_dir)\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=False\n","    )\n","\n","# --- 3. Training and Evaluation Loop for Image-Only Network ---\n","\n","# Hyperparameters for Image-Only Network\n","IO_EPOCHS = 30\n","IO_BATCH_SIZE = 16\n","IO_LR = 0.0005\n","\n","# Setup for 5-fold cross-validation\n","IO_N_FOLDS = 5\n","skf = StratifiedKFold(n_splits=IO_N_FOLDS, shuffle=True, random_state=42)\n","\n","io_results = {\n","    'c_index': [],\n","    'auc': []\n","}\n","\n","print(f\"\\n--- Starting {IO_N_FOLDS}-Fold CV for Image-Only Network ---\")\n","\n","all_ids = filtered_df['patient_id'].values\n","all_labels = filtered_df['treat_response'].values\n","\n","for fold, (train_idx, val_idx) in enumerate(skf.split(all_ids, all_labels)):\n","    print(f\"\\nFold {fold+1}/{IO_N_FOLDS}\")\n","\n","    train_df_fold = filtered_df.iloc[train_idx]\n","    val_df_fold = filtered_df.iloc[val_idx]\n","\n","    train_loader = make_image_loader(train_df_fold, PATCH_DIR, batch_size=IO_BATCH_SIZE)\n","    val_loader = make_image_loader(val_df_fold, PATCH_DIR, shuffle=False, batch_size=len(val_df_fold))\n","\n","    model = ImageOnlyNetwork(img_dim=512).to(device) # img_dim is 512 for each patch\n","    optimizer = torch.optim.Adam(model.parameters(), lr=IO_LR, weight_decay=1e-4)\n","    bce_loss = nn.BCEWithLogitsLoss() # Ensure bce_loss is available\n","\n","    for epoch in range(IO_EPOCHS):\n","        model.train()\n","        for img_features, time, event, label in train_loader:\n","            img_features = img_features.to(device)\n","            time, event, label = time.to(device), event.to(device), label.to(device)\n","\n","            risk, logits = model(img_features)\n","\n","            loss_surv = cox_ph_loss(risk, time, event) # Ensure cox_ph_loss is available\n","            loss_treat = bce_loss(logits, label)\n","\n","            loss = 0.7 * loss_surv + 0.3 * loss_treat\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","    # --- Evaluation for current fold ---\n","    model.eval()\n","    all_risk, all_probs, all_labels = [], [], []\n","    all_time, all_event = [], []\n","\n","    with torch.no_grad():\n","        for img_features, time, event, label in val_loader:\n","            img_features = img_features.to(device)\n","            risk, logits = model(img_features)\n","\n","            all_risk.extend(risk.cpu().numpy())\n","            all_probs.extend(torch.sigmoid(logits).cpu().numpy())\n","            all_labels.extend(label.cpu().numpy())\n","            all_time.extend(time.numpy())\n","            all_event.extend(event.numpy())\n","\n","    # Calculate C-Index (Survival)\n","    c1 = concordance_index(all_time, -np.array(all_risk), all_event)\n","    c2 = concordance_index(all_time, np.array(all_risk), all_event)\n","    c_score = max(c1, c2)\n","\n","    # Calculate AUC (Treatment Response)\n","    try:\n","        auc_score = roc_auc_score(all_labels, all_probs)\n","        if auc_score < 0.5: auc_score = 1.0 - auc_score\n","    except ValueError:\n","        auc_score = 0.5 # Handle single-class batches gracefully\n","\n","    print(f\"  Image-Only Fold {fold+1} Results -> C-Index: {c_score:.4f} | AUC: {auc_score:.4f}\")\n","\n","    io_results['c_index'].append(c_score)\n","    io_results['auc'].append(auc_score)\n","\n","mean_io_c = np.mean(io_results['c_index'])\n","std_io_c = np.std(io_results['c_index'])\n","mean_io_auc = np.mean(io_results['auc'])\n","std_io_auc = np.std(io_results['auc'])\n","\n","print(f\"\\n--- Image-Only Network Final Results ---\")\n","print(f\"  Mean C-Index: {mean_io_c:.4f} \\u00B1 {std_io_c:.4f}\")\n","print(f\"  Mean AUC: {mean_io_auc:.4f} \\u00B1 {std_io_auc:.4f}\")\n","\n","# Update baseline_results dictionary\n","baseline_results['Image-Only Network'] = {\n","    'Modalities': 'Image',\n","    'C-index': f\"{mean_io_c:.4f} \\u00B1 {std_io_c:.4f}\",\n","    'pCR AUC': f\"{mean_io_auc:.4f} \\u00B1 {std_io_auc:.4f}\"\n","}\n","\n","print(\"\\nUpdated Baseline Results:\")\n","print(baseline_results)\n"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Starting 5-Fold CV for Image-Only Network ---\n","\n","Fold 1/5\n","  Image-Only Fold 1 Results -> C-Index: 0.6729 | AUC: 0.5263\n","\n","Fold 2/5\n","  Image-Only Fold 2 Results -> C-Index: 0.5180 | AUC: 0.5789\n","\n","Fold 3/5\n","  Image-Only Fold 3 Results -> C-Index: 0.5342 | AUC: 0.5556\n","\n","Fold 4/5\n","  Image-Only Fold 4 Results -> C-Index: 0.5000 | AUC: 0.5079\n","\n","Fold 5/5\n","  Image-Only Fold 5 Results -> C-Index: 0.5203 | AUC: 0.5556\n","\n","--- Image-Only Network Final Results ---\n","  Mean C-Index: 0.5491 ¬± 0.0629\n","  Mean AUC: 0.5449 ¬± 0.0249\n","\n","Updated Baseline Results:\n","{'Cox PH (Clinical Only)': {'Modalities': 'Clinical', 'C-index': '0.4109 ¬± 0.0744', 'pCR AUC': 'N/A'}, 'DeepSurv (Clinical Only)': {'Modalities': 'Clinical', 'C-index': '0.6148 ¬± 0.1137', 'pCR AUC': '0.6801 ¬± 0.1195'}, 'Image-Only Network': {'Modalities': 'Image', 'C-index': '0.5491 ¬± 0.0629', 'pCR AUC': '0.5449 ¬± 0.0249'}}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3a49b24a","executionInfo":{"status":"ok","timestamp":1768326386179,"user_tz":-330,"elapsed":296,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}},"outputId":"e418ae52-2461-4863-9952-bf741903e994"},"source":["print(\"\\n--- Final Aggregated Baseline Results ---\")\n","for model_name, metrics in baseline_results.items():\n","    print(f\"\\nModel: {model_name}\")\n","    for metric_name, value in metrics.items():\n","        print(f\"  {metric_name}: {value}\")"],"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Final Aggregated Baseline Results ---\n","\n","Model: Cox PH (Clinical Only)\n","  Modalities: Clinical\n","  C-index: 0.4109 ¬± 0.0744\n","  pCR AUC: N/A\n","\n","Model: DeepSurv (Clinical Only)\n","  Modalities: Clinical\n","  C-index: 0.6148 ¬± 0.1137\n","  pCR AUC: 0.6801 ¬± 0.1195\n","\n","Model: Image-Only Network\n","  Modalities: Image\n","  C-index: 0.5491 ¬± 0.0629\n","  pCR AUC: 0.5449 ¬± 0.0249\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88d56c00","executionInfo":{"status":"ok","timestamp":1768326429960,"user_tz":-330,"elapsed":61,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}},"outputId":"cd318c5b-a560-4d4d-fb37-2e880f38043f"},"source":["class NaiveFusionNetwork(nn.Module):\n","    def __init__(self, clin_dim, img_patch_dim=512, hidden_size=128, dropout=0.2):\n","        super().__init__()\n","\n","        # Project clinical features to hidden_size\n","        self.clin_proj = nn.Sequential(\n","            nn.Linear(clin_dim, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # Project image patch features (after averaging) to hidden_size\n","        self.img_proj = nn.Sequential(\n","            nn.Linear(img_patch_dim, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # MLP for concatenated features\n","        self.fusion_mlp = nn.Sequential(\n","            nn.Linear(hidden_size * 2, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_size, hidden_size // 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # Survival Head\n","        self.surv_head = nn.Linear(hidden_size // 2, 1)\n","\n","        # Treatment Response Head\n","        self.treat_head = nn.Linear(hidden_size // 2, 1)\n","\n","    def forward(self, clin, img_patches):\n","        # A. Process clinical features\n","        clin_processed = self.clin_proj(clin)\n","\n","        # B. Average image patches and process image features\n","        # img_patches shape: (batch_size, num_patches, img_patch_dim)\n","        img_avg = img_patches.mean(dim=1) # (batch_size, img_patch_dim)\n","        img_processed = self.img_proj(img_avg)\n","\n","        # C. Concatenate features\n","        fused_features = torch.cat((clin_processed, img_processed), dim=1)\n","\n","        # D. Pass through fusion MLP\n","        mlp_output = self.fusion_mlp(fused_features)\n","\n","        # E. Predict survival and treatment response\n","        risk = self.surv_head(mlp_output).squeeze(-1)\n","        logits = self.treat_head(mlp_output).squeeze(-1)\n","\n","        return risk, logits\n","\n","print(\"NaiveFusionNetwork class defined.\")"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["NaiveFusionNetwork class defined.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29307c35","executionInfo":{"status":"ok","timestamp":1768337009089,"user_tz":-330,"elapsed":10549134,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}},"outputId":"1b448212-516b-47b0-fd8c-a1302ccae066"},"source":["N_FOLDS_NF = 5\n","NF_EPOCHS = 25 # Slightly more epochs for potentially complex fusion\n","NF_BATCH_SIZE = 16\n","NF_LR = 0.0002\n","NF_HIDDEN_SIZE = 64 # Match d_model from best performing RadiogenomicTransformer\n","NF_DROPOUT = 0.2\n","\n","skf = StratifiedKFold(n_splits=N_FOLDS_NF, shuffle=True, random_state=42)\n","\n","nf_results = {\n","    'c_index': [],\n","    'auc': []\n","}\n","\n","print(f\"\\n--- Starting {N_FOLDS_NF}-Fold CV for Naive Fusion (Concatenation MLP) ---\")\n","\n","# Ensure clinical_input_df and filtered_df are available from previous steps\n","clin_dim_nf = clinical_input_df.shape[1] # Clinical features dimension\n","img_patch_dim_nf = 512 # Each image patch is 512-dim\n","\n","all_ids_nf = filtered_df['patient_id'].values\n","all_labels_nf = filtered_df['treat_response'].values # Stratify by Treatment Response\n","\n","for fold, (train_idx, val_idx) in enumerate(skf.split(all_ids_nf, all_labels_nf)):\n","    print(f\"\\nFold {fold+1}/{N_FOLDS_NF}\")\n","\n","    # 1. Split & Loaders (using RadiogenomicPatchDataset which handles image patches)\n","    train_df_fold_nf = filtered_df.iloc[train_idx]\n","    val_df_fold_nf = filtered_df.iloc[val_idx]\n","\n","    train_ds_nf = RadiogenomicPatchDataset(train_df_fold_nf, clinical_input_df, PATCH_DIR, max_patches=50, is_train=True)\n","    val_ds_nf = RadiogenomicPatchDataset(val_df_fold_nf, clinical_input_df, PATCH_DIR, max_patches=50, is_train=False)\n","\n","    train_loader_nf = DataLoader(train_ds_nf, batch_size=NF_BATCH_SIZE, shuffle=True)\n","    val_loader_nf = DataLoader(val_ds_nf, batch_size=len(val_ds_nf), shuffle=False)\n","\n","    # 2. Model Init\n","    model_nf = NaiveFusionNetwork(\n","        clin_dim=clin_dim_nf,\n","        img_patch_dim=img_patch_dim_nf,\n","        hidden_size=NF_HIDDEN_SIZE,\n","        dropout=NF_DROPOUT\n","    ).to(device)\n","    optimizer_nf = torch.optim.Adam(model_nf.parameters(), lr=NF_LR, weight_decay=1e-4)\n","    bce_loss_nf = nn.BCEWithLogitsLoss()\n","\n","    # 3. Training Loop\n","    for epoch in range(NF_EPOCHS):\n","        model_nf.train()\n","        for clin, img_patches, time, event, label in train_loader_nf:\n","            clin, img_patches = clin.to(device), img_patches.to(device)\n","            time, event, label = time.to(device), event.to(device), label.to(device).float()\n","\n","            risk, logits = model_nf(clin, img_patches)\n","\n","            # Multi-Task Loss: 70% Survival, 30% Treatment (adjust alpha as needed)\n","            loss = 0.7 * cox_ph_loss(risk, time, event) + 0.3 * bce_loss_nf(logits, label)\n","\n","            optimizer_nf.zero_grad()\n","            loss.backward()\n","            optimizer_nf.step()\n","\n","    # 4. Final Evaluation of this Fold\n","    model_nf.eval()\n","    all_risk_nf, all_probs_nf, all_labels_nf = [], [], []\n","    all_time_nf, all_event_nf = [], []\n","\n","    with torch.no_grad():\n","        for clin, img_patches, time, event, label in val_loader_nf:\n","            clin, img_patches = clin.to(device), img_patches.to(device)\n","            risk, logits = model_nf(clin, img_patches)\n","\n","            all_risk_nf.extend(risk.cpu().numpy())\n","            all_probs_nf.extend(torch.sigmoid(logits).cpu().numpy()) # Convert logits to 0-1 prob\n","            all_labels_nf.extend(label.cpu().numpy())\n","            all_time_nf.extend(time.numpy())\n","            all_event_nf.extend(event.numpy())\n","\n","    # Calculate C-Index (Survival)\n","    c1_nf = concordance_index(all_time_nf, -np.array(all_risk_nf), all_event_nf)\n","    c2_nf = concordance_index(all_time_nf, np.array(all_risk_nf), all_event_nf)\n","    c_score_nf = max(c1_nf, c2_nf)\n","\n","    # Calculate AUC (Treatment)\n","    try:\n","        auc_score_nf = roc_auc_score(all_labels_nf, all_probs_nf)\n","        # Auto-fix sign flip for AUC\n","        if auc_score_nf < 0.5: auc_score_nf = 1.0 - auc_score_nf\n","    except ValueError:\n","        auc_score_nf = 0.5 # Fail-safe for single-class batches\n","\n","    print(f\"  Naive Fusion Fold {fold+1} Results -> C-Index: {c_score_nf:.4f} | AUC: {auc_score_nf:.4f}\")\n","\n","    nf_results['c_index'].append(c_score_nf)\n","    nf_results['auc'].append(auc_score_nf)\n","\n","# --- FINAL SUMMARY ---\n","mean_nf_c = np.mean(nf_results['c_index'])\n","std_nf_c = np.std(nf_results['c_index'])\n","mean_nf_auc = np.mean(nf_results['auc'])\n","std_nf_auc = np.std(nf_results['auc'])\n","\n","print(f\"\\n--- Naive Fusion (Concatenation MLP) Final Results ---\")\n","print(f\"  Mean C-Index: {mean_nf_c:.4f} \\u00B1 {std_nf_c:.4f}\")\n","print(f\"  Mean AUC: {mean_nf_auc:.4f} \\u00B1 {std_nf_auc:.4f}\")\n","\n","# Update baseline_results dictionary\n","baseline_results['Naive Fusion (Concatenation MLP)'] = {\n","    'Modalities': 'Clinical + Image',\n","    'C-index': f\"{mean_nf_c:.4f} \\u00B1 {std_nf_c:.4f}\",\n","    'pCR AUC': f\"{mean_nf_auc:.4f} \\u00B1 {std_nf_auc:.4f}\"\n","}\n","\n","print(\"\\nUpdated Baseline Results:\")\n","print(baseline_results)\n"],"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Starting 5-Fold CV for Naive Fusion (Concatenation MLP) ---\n","\n","Fold 1/5\n","  Naive Fusion Fold 1 Results -> C-Index: 0.7103 | AUC: 0.7719\n","\n","Fold 2/5\n","  Naive Fusion Fold 2 Results -> C-Index: 0.5586 | AUC: 0.6842\n","\n","Fold 3/5\n","  Naive Fusion Fold 3 Results -> C-Index: 0.6096 | AUC: 0.8651\n","\n","Fold 4/5\n","  Naive Fusion Fold 4 Results -> C-Index: 0.5328 | AUC: 0.5000\n","\n","Fold 5/5\n","  Naive Fusion Fold 5 Results -> C-Index: 0.6504 | AUC: 0.5278\n","\n","--- Naive Fusion (Concatenation MLP) Final Results ---\n","  Mean C-Index: 0.6123 ¬± 0.0637\n","  Mean AUC: 0.6698 ¬± 0.1398\n","\n","Updated Baseline Results:\n","{'Cox PH (Clinical Only)': {'Modalities': 'Clinical', 'C-index': '0.4109 ¬± 0.0744', 'pCR AUC': 'N/A'}, 'DeepSurv (Clinical Only)': {'Modalities': 'Clinical', 'C-index': '0.6148 ¬± 0.1137', 'pCR AUC': '0.6801 ¬± 0.1195'}, 'Image-Only Network': {'Modalities': 'Image', 'C-index': '0.5491 ¬± 0.0629', 'pCR AUC': '0.5449 ¬± 0.0249'}, 'Naive Fusion (Concatenation MLP)': {'Modalities': 'Clinical + Image', 'C-index': '0.6123 ¬± 0.0637', 'pCR AUC': '0.6698 ¬± 0.1398'}}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b89eb39b","executionInfo":{"status":"ok","timestamp":1768337047326,"user_tz":-330,"elapsed":10,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}},"outputId":"7f5d09ad-efe4-4aa6-c53e-2b48ad4a7a12"},"source":["print(\"\\n--- Final Aggregated Baseline Results ---\")\n","for model_name, metrics in baseline_results.items():\n","    print(f\"\\nModel: {model_name}\")\n","    for metric_name, value in metrics.items():\n","        print(f\"  {metric_name}: {value}\")"],"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Final Aggregated Baseline Results ---\n","\n","Model: Cox PH (Clinical Only)\n","  Modalities: Clinical\n","  C-index: 0.4109 ¬± 0.0744\n","  pCR AUC: N/A\n","\n","Model: DeepSurv (Clinical Only)\n","  Modalities: Clinical\n","  C-index: 0.6148 ¬± 0.1137\n","  pCR AUC: 0.6801 ¬± 0.1195\n","\n","Model: Image-Only Network\n","  Modalities: Image\n","  C-index: 0.5491 ¬± 0.0629\n","  pCR AUC: 0.5449 ¬± 0.0249\n","\n","Model: Naive Fusion (Concatenation MLP)\n","  Modalities: Clinical + Image\n","  C-index: 0.6123 ¬± 0.0637\n","  pCR AUC: 0.6698 ¬± 0.1398\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9fe67ba3","executionInfo":{"status":"ok","timestamp":1768337124477,"user_tz":-330,"elapsed":43,"user":{"displayName":"Rishika Ray","userId":"02079523635456342422"}},"outputId":"fadc5889-fabd-4054-aa07-86524c9e29d2"},"source":["radiogenomic_transformer_c_index_mean = np.mean(results['c_index'])\n","radiogenomic_transformer_c_index_std = np.std(results['c_index'])\n","radiogenomic_transformer_auc_mean = np.mean(results['auc'])\n","radiogenomic_transformer_auc_std = np.std(results['auc'])\n","\n","baseline_results['Radiogenomic Transformer'] = {\n","    'Modalities': 'Clinical + Image',\n","    'C-index': f\"{radiogenomic_transformer_c_index_mean:.4f} \\u00B1 {radiogenomic_transformer_c_index_std:.4f}\",\n","    'pCR AUC': f\"{radiogenomic_transformer_auc_mean:.4f} \\u00B1 {radiogenomic_transformer_auc_std:.4f}\"\n","}\n","\n","print(\"Updated Baseline Results with Radiogenomic Transformer:\")\n","print(baseline_results)"],"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated Baseline Results with Radiogenomic Transformer:\n","{'Cox PH (Clinical Only)': {'Modalities': 'Clinical', 'C-index': '0.4109 ¬± 0.0744', 'pCR AUC': 'N/A'}, 'DeepSurv (Clinical Only)': {'Modalities': 'Clinical', 'C-index': '0.6148 ¬± 0.1137', 'pCR AUC': '0.6801 ¬± 0.1195'}, 'Image-Only Network': {'Modalities': 'Image', 'C-index': '0.5491 ¬± 0.0629', 'pCR AUC': '0.5449 ¬± 0.0249'}, 'Naive Fusion (Concatenation MLP)': {'Modalities': 'Clinical + Image', 'C-index': '0.6123 ¬± 0.0637', 'pCR AUC': '0.6698 ¬± 0.1398'}, 'Radiogenomic Transformer': {'Modalities': 'Clinical + Image', 'C-index': '0.5874 ¬± 0.0551', 'pCR AUC': '0.6617 ¬± 0.0940'}}\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOpPkuUi/wKlPlNLfIAIQSb"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}